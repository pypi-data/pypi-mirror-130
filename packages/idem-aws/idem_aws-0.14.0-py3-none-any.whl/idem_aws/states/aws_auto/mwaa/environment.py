"""
Autogenerated state module using `pop-create-idem <https://gitlab.com/saltstack/pop/pop-create-idem>`__

hub.exec.boto3.client.mwaa.create_environment
hub.exec.boto3.client.mwaa.delete_environment
hub.exec.boto3.client.mwaa.get_environment
hub.exec.boto3.client.mwaa.list_environments
hub.exec.boto3.client.mwaa.update_environment
"""



from typing import *
import dict_tools.differ as differ
async def present(hub, ctx, name: Text, dag_s3_path: Text, execution_role_arn: Text, network_configuration: Dict, source_bucket_arn: Text, airflow_configuration_options: Dict = None, airflow_version: Text = None, environment_class: Text = None, kms_key: Text = None, logging_configuration: Dict = None, max_workers: int = None, min_workers: int = None, plugins_s3_object_version: Text = None, plugins_s3_path: Text = None, requirements_s3_object_version: Text = None, requirements_s3_path: Text = None, schedulers: int = None, tags: Dict = None, webserver_access_mode: Text = None, weekly_maintenance_window_start: Text = None)  -> Dict[str, Any]:
    r'''
    **Autogenerated function**
    
    Creates an Amazon Managed Workflows for Apache Airflow (MWAA) environment.

    Args:
        name(Text): The name of the Amazon MWAA environment. For example, MyMWAAEnvironment.
        airflow_configuration_options(Dict, optional): A list of key-value pairs containing the Apache Airflow configuration options you want to attach
            to your environment. To learn more, see Apache Airflow configuration options. Defaults to None.
        airflow_version(Text, optional): The Apache Airflow version for your environment. For example, v1.10.12. If no value is
            specified, defaults to the latest version. Valid values: v1.10.12. Defaults to None.
        dag_s3_path(Text): The relative path to the DAGs folder on your Amazon S3 bucket. For example, dags. To learn more,
            see Adding or updating DAGs.
        environment_class(Text, optional): The environment class type. Valid values: mw1.small, mw1.medium, mw1.large. To learn more, see
            Amazon MWAA environment class. Defaults to None.
        execution_role_arn(Text): The Amazon Resource Name (ARN) of the execution role for your environment. An execution role is
            an AWS Identity and Access Management (IAM) role that grants MWAA permission to access AWS
            services and resources used by your environment. For example, arn:aws:iam::123456789:role/my-
            execution-role. To learn more, see Amazon MWAA Execution role.
        kms_key(Text, optional): The AWS Key Management Service (KMS) key to encrypt the data in your environment. You can use an
            AWS owned CMK, or a Customer managed CMK (advanced). To learn more, see Get started with Amazon
            Managed Workflows for Apache Airflow. Defaults to None.
        logging_configuration(Dict, optional): Defines the Apache Airflow logs to send to CloudWatch Logs: DagProcessingLogs, SchedulerLogs,
            TaskLogs, WebserverLogs, WorkerLogs. Defaults to None.
        max_workers(int, optional): The maximum number of workers that you want to run in your environment. MWAA scales the number
            of Apache Airflow workers up to the number you specify in the MaxWorkers field. For example, 20.
            When there are no more tasks running, and no more in the queue, MWAA disposes of the extra
            workers leaving the one worker that is included with your environment, or the number you specify
            in MinWorkers. Defaults to None.
        min_workers(int, optional): The minimum number of workers that you want to run in your environment. MWAA scales the number
            of Apache Airflow workers up to the number you specify in the MaxWorkers field. When there are
            no more tasks running, and no more in the queue, MWAA disposes of the extra workers leaving the
            worker count you specify in the MinWorkers field. For example, 2. Defaults to None.
        network_configuration(Dict): The VPC networking components used to secure and enable network traffic between the AWS
            resources for your environment. To learn more, see About networking on Amazon MWAA.
        plugins_s3_object_version(Text, optional): The version of the plugins.zip file on your Amazon S3 bucket. A version must be specified each
            time a plugins.zip file is updated. To learn more, see How S3 Versioning works. Defaults to None.
        plugins_s3_path(Text, optional): The relative path to the plugins.zip file on your Amazon S3 bucket. For example, plugins.zip. If
            specified, then the plugins.zip version is required. To learn more, see Installing custom
            plugins. Defaults to None.
        requirements_s3_object_version(Text, optional): The version of the requirements.txt file on your Amazon S3 bucket. A version must be specified
            each time a requirements.txt file is updated. To learn more, see How S3 Versioning works. Defaults to None.
        requirements_s3_path(Text, optional): The relative path to the requirements.txt file on your Amazon S3 bucket. For example,
            requirements.txt. If specified, then a file version is required. To learn more, see Installing
            Python dependencies. Defaults to None.
        schedulers(int, optional): The number of Apache Airflow schedulers to run in your environment. Defaults to None.
        source_bucket_arn(Text): The Amazon Resource Name (ARN) of the Amazon S3 bucket where your DAG code and supporting files
            are stored. For example, arn:aws:s3:::my-airflow-bucket-unique-name. To learn more, see Create
            an Amazon S3 bucket for Amazon MWAA.
        tags(Dict, optional): The key-value tag pairs you want to associate to your environment. For example, "Environment":
            "Staging". To learn more, see Tagging AWS resources. Defaults to None.
        webserver_access_mode(Text, optional): The Apache Airflow Web server access mode. To learn more, see Apache Airflow access modes. Defaults to None.
        weekly_maintenance_window_start(Text, optional): The day and time of the week to start weekly maintenance updates of your environment in the
            following format: DAY:HH:MM. For example: TUE:03:30. You can specify a start time in 30 minute
            increments only. Supported input includes the following:
            MON|TUE|WED|THU|FRI|SAT|SUN:([01]\\d|2[0-3]):(00|30). Defaults to None.

    Returns:
        Dict[str, Any]

    Examples:

        .. code-block:: sls

            resource_is_present:
              aws_auto.mwaa.environment.present:
                - name: value
                - dag_s3_path: value
                - execution_role_arn: value
                - network_configuration: value
                - source_bucket_arn: value
    '''
    
    result = dict(comment="", changes= None, name=name, result=True)
    ret = await hub.exec.boto3.client.mwaa.environment.id(
        ctx,
        jmes_path=name
    )
    if ret["status"]:
        # name is now the first id that matched the JMES search path
        name = ret["ret"]
    

    
    before = await hub.exec.boto3.client.mwaa.get_environment(name)

    if before:
        result["comment"] = f"'{name}' already exists"
    else:
        try:
            ret = await hub.exec.boto3.client.mwaa.create_environment(
                ctx,
                
                
                **{"Name": name, "AirflowConfigurationOptions": airflow_configuration_options, "AirflowVersion": airflow_version, "DagS3Path": dag_s3_path, "EnvironmentClass": environment_class, "ExecutionRoleArn": execution_role_arn, "KmsKey": kms_key, "LoggingConfiguration": logging_configuration, "MaxWorkers": max_workers, "MinWorkers": min_workers, "NetworkConfiguration": network_configuration, "PluginsS3ObjectVersion": plugins_s3_object_version, "PluginsS3Path": plugins_s3_path, "RequirementsS3ObjectVersion": requirements_s3_object_version, "RequirementsS3Path": requirements_s3_path, "Schedulers": schedulers, "SourceBucketArn": source_bucket_arn, "Tags": tags, "WebserverAccessMode": webserver_access_mode, "WeeklyMaintenanceWindowStart": weekly_maintenance_window_start}
            )
            result["result"] = ret["status"]
            if not result["result"]:
                result["comment"] = ret["comment"]
                return result
            ret["comment"] = f"Created '{name}'"
        except hub.tool.boto3.exception.ClientError as e:
            result["comment"] = f"{e.__class__.__name__}: {e}"

    
    # TODO perform other modifications as needed here
    ...

    after = await hub.exec.boto3.client.mwaa.get_environment(name)
    result["changes"] = differ.deep_diff(before, after)
    return result

async def absent(hub, ctx, name: Text)  -> Dict[str, Any]:
    r'''
    **Autogenerated function**
    
    Deletes an Amazon Managed Workflows for Apache Airflow (MWAA) environment.

    Args:
        name(Text): The name of the Amazon MWAA environment. For example, MyMWAAEnvironment.

    Returns:
        Dict[str, Any]

    Examples:

        .. code-block:: sls

            resource_is_absent:
              aws_auto.mwaa.environment.absent:
                - name: value
    '''
    
    result = dict(comment="", changes= None, name=name, result=True)
    ret = await hub.exec.boto3.client.mwaa.environment.id(
        ctx,
        jmes_path=name
    )
    if ret["status"]:
        # name is now the first id that matched the JMES search path
        name = ret["ret"]
    

    

    before = await hub.exec.boto3.client.mwaa.get_environment(name)

    if not before:
        result["comment"] = f"'{name}' already absent"
    else:
        try:
            ret = await hub.exec.boto3.client.mwaa.delete_environment(
                ctx,
                
                
                **{"Name": name}
            )
            result["result"] = ret["status"]
            if not result["result"]:
                result["comment"] = ret["comment"]
                return result
            result["comment"] = f"Deleted '{name}'"
        except hub.tool.boto3.exception.ClientError as e:
            result["comment"] = f"{e.__class__.__name__}: {e}"

    

    after = await hub.exec.boto3.client.mwaa.get_environment(name)
    result["changes"] = differ.deep_diff(before, after)
    return result

