"""
Autogenerated state module using `pop-create-idem <https://gitlab.com/saltstack/pop/pop-create-idem>`__

hub.exec.boto3.client.firehose.create_delivery_stream
hub.exec.boto3.client.firehose.delete_delivery_stream
hub.exec.boto3.client.firehose.describe_delivery_stream
hub.exec.boto3.client.firehose.list_delivery_streams
hub.exec.boto3.client.firehose.tag_delivery_stream
hub.exec.boto3.client.firehose.untag_delivery_stream
"""



from typing import *
import dict_tools.differ as differ
async def present(hub, ctx, name: Text, delivery_stream_name: Text, delivery_stream_type: Text = None, kinesis_stream_source_configuration: Dict = None, delivery_stream_encryption_configuration_input: Dict = None, s3_destination_configuration: Dict = None, extended_s3_destination_configuration: Dict = None, redshift_destination_configuration: Dict = None, elasticsearch_destination_configuration: Dict = None, splunk_destination_configuration: Dict = None, http_endpoint_destination_configuration: Dict = None, tags: List = None)  -> Dict[str, Any]:
    r'''
    **Autogenerated function**
    
    Creates a Kinesis Data Firehose delivery stream. By default, you can create up to 50 delivery streams per AWS
    Region. This is an asynchronous operation that immediately returns. The initial status of the delivery stream is
    CREATING. After the delivery stream is created, its status is ACTIVE and it now accepts data. If the delivery
    stream creation fails, the status transitions to CREATING_FAILED. Attempts to send data to a delivery stream
    that is not in the ACTIVE state cause an exception. To check the state of a delivery stream, use
    DescribeDeliveryStream. If the status of a delivery stream is CREATING_FAILED, this status doesn't change, and
    you can't invoke CreateDeliveryStream again on it. However, you can invoke the DeleteDeliveryStream operation to
    delete it. A Kinesis Data Firehose delivery stream can be configured to receive records directly from providers
    using PutRecord or PutRecordBatch, or it can be configured to use an existing Kinesis stream as its source. To
    specify a Kinesis data stream as input, set the DeliveryStreamType parameter to KinesisStreamAsSource, and
    provide the Kinesis stream Amazon Resource Name (ARN) and role ARN in the KinesisStreamSourceConfiguration
    parameter. To create a delivery stream with server-side encryption (SSE) enabled, include
    DeliveryStreamEncryptionConfigurationInput in your request. This is optional. You can also invoke
    StartDeliveryStreamEncryption to turn on SSE for an existing delivery stream that doesn't have SSE enabled. A
    delivery stream is configured with a single destination: Amazon S3, Amazon ES, Amazon Redshift, or Splunk. You
    must specify only one of the following destination configuration parameters: ExtendedS3DestinationConfiguration,
    S3DestinationConfiguration, ElasticsearchDestinationConfiguration, RedshiftDestinationConfiguration, or
    SplunkDestinationConfiguration. When you specify S3DestinationConfiguration, you can also provide the following
    optional values: BufferingHints, EncryptionConfiguration, and CompressionFormat. By default, if no
    BufferingHints value is provided, Kinesis Data Firehose buffers data up to 5 MB or for 5 minutes, whichever
    condition is satisfied first. BufferingHints is a hint, so there are some cases where the service cannot adhere
    to these conditions strictly. For example, record boundaries might be such that the size is a little over or
    under the configured buffering size. By default, no encryption is performed. We strongly recommend that you
    enable encryption to ensure secure data storage in Amazon S3. A few notes about Amazon Redshift as a
    destination:   An Amazon Redshift destination requires an S3 bucket as intermediate location. Kinesis Data
    Firehose first delivers data to Amazon S3 and then uses COPY syntax to load data into an Amazon Redshift table.
    This is specified in the RedshiftDestinationConfiguration.S3Configuration parameter.   The compression formats
    SNAPPY or ZIP cannot be specified in RedshiftDestinationConfiguration.S3Configuration because the Amazon
    Redshift COPY operation that reads from the S3 bucket doesn't support these compression formats.   We strongly
    recommend that you use the user name and password you provide exclusively with Kinesis Data Firehose, and that
    the permissions for the account are restricted for Amazon Redshift INSERT permissions.   Kinesis Data Firehose
    assumes the IAM role that is configured as part of the destination. The role should allow the Kinesis Data
    Firehose principal to assume the role, and the role should have permissions that allow the service to deliver
    the data. For more information, see Grant Kinesis Data Firehose Access to an Amazon S3 Destination in the Amazon
    Kinesis Data Firehose Developer Guide.

    Args:
        name(Text): A name, ID, or JMES search path to identify the resource.
        delivery_stream_name(Text): The name of the delivery stream. This name must be unique per AWS account in the same AWS
            Region. If the delivery streams are in different accounts or different Regions, you can have
            multiple delivery streams with the same name.
        delivery_stream_type(Text, optional): The delivery stream type. This parameter can be one of the following values:    DirectPut:
            Provider applications access the delivery stream directly.    KinesisStreamAsSource: The
            delivery stream uses a Kinesis data stream as a source. Defaults to None.
        kinesis_stream_source_configuration(Dict, optional): When a Kinesis data stream is used as the source for the delivery stream, a
            KinesisStreamSourceConfiguration containing the Kinesis data stream Amazon Resource Name (ARN)
            and the role ARN for the source stream. Defaults to None.
        delivery_stream_encryption_configuration_input(Dict, optional): Used to specify the type and Amazon Resource Name (ARN) of the KMS key needed for Server-Side
            Encryption (SSE). Defaults to None.
        s3_destination_configuration(Dict, optional): [Deprecated] The destination in Amazon S3. You can specify only one destination. Defaults to None.
        extended_s3_destination_configuration(Dict, optional): The destination in Amazon S3. You can specify only one destination. Defaults to None.
        redshift_destination_configuration(Dict, optional): The destination in Amazon Redshift. You can specify only one destination. Defaults to None.
        elasticsearch_destination_configuration(Dict, optional): The destination in Amazon ES. You can specify only one destination. Defaults to None.
        splunk_destination_configuration(Dict, optional): The destination in Splunk. You can specify only one destination. Defaults to None.
        http_endpoint_destination_configuration(Dict, optional): Enables configuring Kinesis Firehose to deliver data to any HTTP endpoint destination. You can
            specify only one destination. Defaults to None.
        tags(List, optional): A set of tags to assign to the delivery stream. A tag is a key-value pair that you can define
            and assign to AWS resources. Tags are metadata. For example, you can add friendly names and
            descriptions or other types of information that can help you distinguish the delivery stream.
            For more information about tags, see Using Cost Allocation Tags in the AWS Billing and Cost
            Management User Guide. You can specify up to 50 tags when creating a delivery stream. Defaults to None.

    Returns:
        Dict[str, Any]

    Examples:

        .. code-block:: sls

            resource_is_present:
              aws_auto.firehose.delivery_stream.present:
                - name: value
                - delivery_stream_name: value
    '''
    
    result = dict(comment="", changes= None, name=name, result=True)
    ret = await hub.exec.boto3.client.firehose.delivery_stream.id(
        ctx,
        jmes_path=name
    )
    if ret["status"]:
        # name is now the first id that matched the JMES search path
        name = ret["ret"]
    

    
    before = await hub.exec.boto3.client.firehose.describe_delivery_stream(name)

    if before:
        result["comment"] = f"'{name}' already exists"
    else:
        try:
            ret = await hub.exec.boto3.client.firehose.create_delivery_stream(
                ctx,
                
                
                **{"DeliveryStreamName": delivery_stream_name, "DeliveryStreamType": delivery_stream_type, "KinesisStreamSourceConfiguration": kinesis_stream_source_configuration, "DeliveryStreamEncryptionConfigurationInput": delivery_stream_encryption_configuration_input, "S3DestinationConfiguration": s3_destination_configuration, "ExtendedS3DestinationConfiguration": extended_s3_destination_configuration, "RedshiftDestinationConfiguration": redshift_destination_configuration, "ElasticsearchDestinationConfiguration": elasticsearch_destination_configuration, "SplunkDestinationConfiguration": splunk_destination_configuration, "HttpEndpointDestinationConfiguration": http_endpoint_destination_configuration, "Tags": tags}
            )
            result["result"] = ret["status"]
            if not result["result"]:
                result["comment"] = ret["comment"]
                return result
            ret["comment"] = f"Created '{name}'"
        except hub.tool.boto3.exception.ClientError as e:
            result["comment"] = f"{e.__class__.__name__}: {e}"

    
    # TODO perform other modifications as needed here
    ...

    after = await hub.exec.boto3.client.firehose.describe_delivery_stream(name)
    result["changes"] = differ.deep_diff(before, after)
    return result

async def absent(hub, ctx, name: Text, delivery_stream_name: Text, allow_force_delete: bool = None)  -> Dict[str, Any]:
    r'''
    **Autogenerated function**
    
    Deletes a delivery stream and its data. To check the state of a delivery stream, use DescribeDeliveryStream. You
    can delete a delivery stream only if it is in one of the following states: ACTIVE, DELETING, CREATING_FAILED, or
    DELETING_FAILED. You can't delete a delivery stream that is in the CREATING state. While the deletion request is
    in process, the delivery stream is in the DELETING state. While the delivery stream is in the DELETING state,
    the service might continue to accept records, but it doesn't make any guarantees with respect to delivering the
    data. Therefore, as a best practice, first stop any applications that are sending records before you delete a
    delivery stream.

    Args:
        name(Text): A name, ID, or JMES search path to identify the resource.
        delivery_stream_name(Text): The name of the delivery stream.
        allow_force_delete(bool, optional): Set this to true if you want to delete the delivery stream even if Kinesis Data Firehose is
            unable to retire the grant for the CMK. Kinesis Data Firehose might be unable to retire the
            grant due to a customer error, such as when the CMK or the grant are in an invalid state. If you
            force deletion, you can then use the RevokeGrant operation to revoke the grant you gave to
            Kinesis Data Firehose. If a failure to retire the grant happens due to an AWS KMS issue, Kinesis
            Data Firehose keeps retrying the delete operation. The default value is false. Defaults to None.

    Returns:
        Dict[str, Any]

    Examples:

        .. code-block:: sls

            resource_is_absent:
              aws_auto.firehose.delivery_stream.absent:
                - name: value
                - delivery_stream_name: value
    '''
    
    result = dict(comment="", changes= None, name=name, result=True)
    ret = await hub.exec.boto3.client.firehose.delivery_stream.id(
        ctx,
        jmes_path=name
    )
    if ret["status"]:
        # name is now the first id that matched the JMES search path
        name = ret["ret"]
    

    

    before = await hub.exec.boto3.client.firehose.describe_delivery_stream(name)

    if not before:
        result["comment"] = f"'{name}' already absent"
    else:
        try:
            ret = await hub.exec.boto3.client.firehose.delete_delivery_stream(
                ctx,
                
                
                **{"DeliveryStreamName": delivery_stream_name, "AllowForceDelete": allow_force_delete}
            )
            result["result"] = ret["status"]
            if not result["result"]:
                result["comment"] = ret["comment"]
                return result
            result["comment"] = f"Deleted '{name}'"
        except hub.tool.boto3.exception.ClientError as e:
            result["comment"] = f"{e.__class__.__name__}: {e}"

    

    after = await hub.exec.boto3.client.firehose.describe_delivery_stream(name)
    result["changes"] = differ.deep_diff(before, after)
    return result

