"""
Autogenerated state module using `pop-create-idem <https://gitlab.com/saltstack/pop/pop-create-idem>`__

hub.exec.boto3.client.sagemaker.create_model
hub.exec.boto3.client.sagemaker.delete_model
hub.exec.boto3.client.sagemaker.describe_model
hub.exec.boto3.client.sagemaker.list_models
"""



from typing import *
import dict_tools.differ as differ
async def present(hub, ctx, name: Text, model_name: Text, execution_role_arn: Text, primary_container: Dict = None, containers: List = None, inference_execution_config: Dict = None, tags: List = None, vpc_config: Dict = None, enable_network_isolation: bool = None)  -> Dict[str, Any]:
    r'''
    **Autogenerated function**
    
    Creates a model in Amazon SageMaker. In the request, you name the model and describe a primary container. For
    the primary container, you specify the Docker image that contains inference code, artifacts (from prior
    training), and a custom environment map that the inference code uses when you deploy the model for predictions.
    Use this API to create a model if you want to use Amazon SageMaker hosting services or run a batch transform
    job. To host your model, you create an endpoint configuration with the CreateEndpointConfig API, and then create
    an endpoint with the CreateEndpoint API. Amazon SageMaker then deploys all of the containers that you defined
    for the model in the hosting environment.  For an example that calls this method when deploying a model to
    Amazon SageMaker hosting services, see Deploy the Model to Amazon SageMaker Hosting Services (Amazon Web
    Services SDK for Python (Boto 3)).  To run a batch transform using your model, you start a job with the
    CreateTransformJob API. Amazon SageMaker uses your model and your dataset to get inferences which are then saved
    to a specified S3 location. In the CreateModel request, you must define a container with the PrimaryContainer
    parameter. In the request, you also provide an IAM role that Amazon SageMaker can assume to access model
    artifacts and docker image for deployment on ML compute hosting instances or for batch transform jobs. In
    addition, you also use the IAM role to manage permissions the inference code needs. For example, if the
    inference code access any other Amazon Web Services resources, you grant necessary permissions via this role.

    Args:
        name(Text): A name, ID, or JMES search path to identify the resource.
        model_name(Text): The name of the new model.
        primary_container(Dict, optional): The location of the primary docker image containing inference code, associated artifacts, and
            custom environment map that the inference code uses when the model is deployed for predictions. Defaults to None.
        containers(List, optional): Specifies the containers in the inference pipeline. Defaults to None.
        inference_execution_config(Dict, optional): Specifies details of how containers in a multi-container endpoint are called. Defaults to None.
        execution_role_arn(Text): The Amazon Resource Name (ARN) of the IAM role that Amazon SageMaker can assume to access model
            artifacts and docker image for deployment on ML compute instances or for batch transform jobs.
            Deploying on ML compute instances is part of model hosting. For more information, see Amazon
            SageMaker Roles.   To be able to pass this role to Amazon SageMaker, the caller of this API must
            have the iam:PassRole permission.
        tags(List, optional): An array of key-value pairs. You can use tags to categorize your Amazon Web Services resources
            in different ways, for example, by purpose, owner, or environment. For more information, see
            Tagging Amazon Web Services Resources. Defaults to None.
        vpc_config(Dict, optional): A VpcConfig object that specifies the VPC that you want your model to connect to. Control access
            to and from your model container by configuring the VPC. VpcConfig is used in hosting services
            and in batch transform. For more information, see Protect Endpoints by Using an Amazon Virtual
            Private Cloud and Protect Data in Batch Transform Jobs by Using an Amazon Virtual Private Cloud. Defaults to None.
        enable_network_isolation(bool, optional): Isolates the model container. No inbound or outbound network calls can be made to or from the
            model container. Defaults to None.

    Returns:
        Dict[str, Any]

    Examples:

        .. code-block:: sls

            resource_is_present:
              aws_auto.sagemaker.model.present:
                - name: value
                - model_name: value
                - execution_role_arn: value
    '''
    
    result = dict(comment="", changes= None, name=name, result=True)
    ret = await hub.exec.boto3.client.sagemaker.model.id(
        ctx,
        jmes_path=name
    )
    if ret["status"]:
        # name is now the first id that matched the JMES search path
        name = ret["ret"]
    

    
    before = await hub.exec.boto3.client.sagemaker.describe_model(name)

    if before:
        result["comment"] = f"'{name}' already exists"
    else:
        try:
            ret = await hub.exec.boto3.client.sagemaker.create_model(
                ctx,
                
                
                **{"ModelName": model_name, "PrimaryContainer": primary_container, "Containers": containers, "InferenceExecutionConfig": inference_execution_config, "ExecutionRoleArn": execution_role_arn, "Tags": tags, "VpcConfig": vpc_config, "EnableNetworkIsolation": enable_network_isolation}
            )
            result["result"] = ret["status"]
            if not result["result"]:
                result["comment"] = ret["comment"]
                return result
            ret["comment"] = f"Created '{name}'"
        except hub.tool.boto3.exception.ClientError as e:
            result["comment"] = f"{e.__class__.__name__}: {e}"

    
    # TODO perform other modifications as needed here
    ...

    after = await hub.exec.boto3.client.sagemaker.describe_model(name)
    result["changes"] = differ.deep_diff(before, after)
    return result

async def absent(hub, ctx, name: Text, model_name: Text)  -> Dict[str, Any]:
    r'''
    **Autogenerated function**
    
    Deletes a model. The DeleteModel API deletes only the model entry that was created in Amazon SageMaker when you
    called the CreateModel API. It does not delete model artifacts, inference code, or the IAM role that you
    specified when creating the model.

    Args:
        name(Text): A name, ID, or JMES search path to identify the resource.
        model_name(Text): The name of the model to delete.

    Returns:
        Dict[str, Any]

    Examples:

        .. code-block:: sls

            resource_is_absent:
              aws_auto.sagemaker.model.absent:
                - name: value
                - model_name: value
    '''
    
    result = dict(comment="", changes= None, name=name, result=True)
    ret = await hub.exec.boto3.client.sagemaker.model.id(
        ctx,
        jmes_path=name
    )
    if ret["status"]:
        # name is now the first id that matched the JMES search path
        name = ret["ret"]
    

    

    before = await hub.exec.boto3.client.sagemaker.describe_model(name)

    if not before:
        result["comment"] = f"'{name}' already absent"
    else:
        try:
            ret = await hub.exec.boto3.client.sagemaker.delete_model(
                ctx,
                
                
                **{"ModelName": model_name}
            )
            result["result"] = ret["status"]
            if not result["result"]:
                result["comment"] = ret["comment"]
                return result
            result["comment"] = f"Deleted '{name}'"
        except hub.tool.boto3.exception.ClientError as e:
            result["comment"] = f"{e.__class__.__name__}: {e}"

    

    after = await hub.exec.boto3.client.sagemaker.describe_model(name)
    result["changes"] = differ.deep_diff(before, after)
    return result

