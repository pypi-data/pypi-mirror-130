"""
Autogenerated state module using `pop-create-idem <https://gitlab.com/saltstack/pop/pop-create-idem>`__

hub.exec.boto3.client.ec2.create_flow_logs
hub.exec.boto3.client.ec2.delete_flow_logs
hub.exec.boto3.client.ec2.describe_flow_logs
"""



import copy
from typing import *
import dict_tools.differ as differ

async def present(hub, ctx, name: Text, resource_ids: List, resource_type: Text, traffic_type: Text, deliver_logs_permission_arn: Text = None, log_group_name: Text = None, log_destination_type: Text = None, log_destination: Text = None, log_format: Text = None, tag_specifications: List = None, max_aggregation_interval: int = None)  -> Dict[str, Any]:
    r'''
    **Autogenerated function**
    
    Creates one or more flow logs to capture information about IP traffic for a specific network interface, subnet,
    or VPC.  Flow log data for a monitored network interface is recorded as flow log records, which are log events
    consisting of fields that describe the traffic flow. For more information, see Flow log records in the Amazon
    Virtual Private Cloud User Guide. When publishing to CloudWatch Logs, flow log records are published to a log
    group, and each network interface has a unique log stream in the log group. When publishing to Amazon S3, flow
    log records for all of the monitored network interfaces are published to a single log file object that is stored
    in the specified bucket. For more information, see VPC Flow Logs in the Amazon Virtual Private Cloud User Guide.

    Args:
        name(Text): A name, ID, or JMES search path to identify the resource.
        deliver_logs_permission_arn(Text, optional): The ARN for the IAM role that permits Amazon EC2 to publish flow logs to a CloudWatch Logs log
            group in your account. If you specify LogDestinationType as s3, do not specify
            DeliverLogsPermissionArn or LogGroupName. Defaults to None.
        log_group_name(Text, optional): The name of a new or existing CloudWatch Logs log group where Amazon EC2 publishes your flow
            logs. If you specify LogDestinationType as s3, do not specify DeliverLogsPermissionArn or
            LogGroupName. Defaults to None.
        resource_ids(List): The ID of the subnet, network interface, or VPC for which you want to create a flow log.
            Constraints: Maximum of 1000 resources.
        resource_type(Text): The type of resource for which to create the flow log. For example, if you specified a VPC ID
            for the ResourceId property, specify VPC for this property.
        traffic_type(Text): The type of traffic to log. You can log traffic that the resource accepts or rejects, or all
            traffic.
        log_destination_type(Text, optional): Specifies the type of destination to which the flow log data is to be published. Flow log data
            can be published to CloudWatch Logs or Amazon S3. To publish flow log data to CloudWatch Logs,
            specify cloud-watch-logs. To publish flow log data to Amazon S3, specify s3. If you specify
            LogDestinationType as s3, do not specify DeliverLogsPermissionArn or LogGroupName. Default:
            cloud-watch-logs. Defaults to None.
        log_destination(Text, optional): Specifies the destination to which the flow log data is to be published. Flow log data can be
            published to a CloudWatch Logs log group or an Amazon S3 bucket. The value specified for this
            parameter depends on the value specified for LogDestinationType. If LogDestinationType is not
            specified or cloud-watch-logs, specify the Amazon Resource Name (ARN) of the CloudWatch Logs log
            group. For example, to publish to a log group called my-logs, specify arn:aws:logs:us-
            east-1:123456789012:log-group:my-logs. Alternatively, use LogGroupName instead. If
            LogDestinationType is s3, specify the ARN of the Amazon S3 bucket. You can also specify a
            subfolder in the bucket. To specify a subfolder in the bucket, use the following ARN format:
            bucket_ARN/subfolder_name/. For example, to specify a subfolder named my-logs in a bucket named
            my-bucket, use the following ARN: arn:aws:s3:::my-bucket/my-logs/. You cannot use AWSLogs as a
            subfolder name. This is a reserved term. Defaults to None.
        log_format(Text, optional): The fields to include in the flow log record, in the order in which they should appear. For a
            list of available fields, see Flow log records. If you omit this parameter, the flow log is
            created using the default format. If you specify this parameter, you must specify at least one
            field. Specify the fields using the ${field-id} format, separated by spaces. For the CLI, use
            single quotation marks (' ') to surround the parameter value. Defaults to None.
        tag_specifications(List, optional): The tags to apply to the flow logs. Defaults to None.
        max_aggregation_interval(int, optional): The maximum interval of time during which a flow of packets is captured and aggregated into a
            flow log record. You can specify 60 seconds (1 minute) or 600 seconds (10 minutes). When a
            network interface is attached to a Nitro-based instance, the aggregation interval is always 60
            seconds or less, regardless of the value that you specify. Default: 600. Defaults to None.

    Returns:
        Dict[str, Any]

    Examples:

        .. code-block:: sls

            resource_is_present:
              aws_auto.ec2.flow_log.present:
                - name: value
                - resource_ids: value
                - resource_type: value
                - traffic_type: value
    '''

    
    result = dict(comment="", changes= None, name=name, result=True)
    ret = await hub.exec.boto3.client.ec2.flow_log.id(
        ctx,
        jmes_path=name
    )
    if ret["status"]:
        # name is now the first id that matched the JMES search path
        name = ret["ret"]
    

    
    before = await hub.exec.boto3.client.ec2.describe_flow_logs(name)

    if before:
        result["comment"] = f"'{name}' already exists"
    else:
        try:
            ret = await hub.exec.boto3.client.ec2.create_flow_logs(
                ctx,
                DryRun=ctx.test,
                ClientToken=name,
                **{"DeliverLogsPermissionArn": deliver_logs_permission_arn, "LogGroupName": log_group_name, "ResourceIds": resource_ids, "ResourceType": resource_type, "TrafficType": traffic_type, "LogDestinationType": log_destination_type, "LogDestination": log_destination, "LogFormat": log_format, "TagSpecifications": tag_specifications, "MaxAggregationInterval": max_aggregation_interval}
            )
            result["result"] = ret["status"]
            if not result["result"]:
                result["comment"] = ret["comment"]
                return result
            ret["comment"] = f"Created '{name}'"
        except hub.tool.boto3.exception.ClientError as e:
            result["comment"] = f"{e.__class__.__name__}: {e}"

    
    # TODO perform other modifications as needed here
    ...

    after = await hub.exec.boto3.client.ec2.describe_flow_logs(name)
    result["changes"] = differ.deep_diff(before, after)
    return result



async def absent(hub, ctx, name: Text, flow_log_ids: List)  -> Dict[str, Any]:
    r'''
    **Autogenerated function**
    
    Deletes one or more flow logs.

    Args:
        name(Text): A name, ID, or JMES search path to identify the resource.
        flow_log_ids(List): One or more flow log IDs. Constraint: Maximum of 1000 flow log IDs.

    Returns:
        Dict[str, Any]

    Examples:

        .. code-block:: sls

            resource_is_absent:
              aws_auto.ec2.flow_log.absent:
                - name: value
                - flow_log_ids: value
    '''

    
    result = dict(comment="", changes= None, name=name, result=True)
    ret = await hub.exec.boto3.client.ec2.flow_log.id(
        ctx,
        jmes_path=name
    )
    if ret["status"]:
        # name is now the first id that matched the JMES search path
        name = ret["ret"]
    

    

    before = await hub.exec.boto3.client.ec2.describe_flow_logs(name)

    if not before:
        result["comment"] = f"'{name}' already absent"
    else:
        try:
            ret = await hub.exec.boto3.client.ec2.delete_flow_logs(
                ctx,
                DryRun=ctx.test,
                
                **{"FlowLogIds": flow_log_ids}
            )
            result["result"] = ret["status"]
            if not result["result"]:
                result["comment"] = ret["comment"]
                return result
            result["comment"] = f"Deleted '{name}'"
        except hub.tool.boto3.exception.ClientError as e:
            result["comment"] = f"{e.__class__.__name__}: {e}"

    

    after = await hub.exec.boto3.client.ec2.describe_flow_logs(name)
    result["changes"] = differ.deep_diff(before, after)
    return result



async def describe(hub, ctx)  -> Dict[str, Dict[str, Any]]:
    r'''
    **Autogenerated function**

    Describe the resource in a way that can be recreated/managed with the corresponding "present" function

    
    Describes one or more flow logs. To view the information in your flow logs (the log streams for the network
    interfaces), you must use the CloudWatch Logs console or the CloudWatch Logs API.


    Returns:
        Dict[str, Any]

    Examples:

        .. code-block:: bash

            $ idem describe aws_auto.ec2.flow_log
    '''

    
    result = {}

    ret = await hub.exec.boto3.client.ec2.describe_flow_logs(ctx)
    if not ret["status"]:
        hub.log.debug(f"Could not describe flow_log {ret['comment']}")
        return result

    for flow_log in ret["ret"]["TODO"]:
        new_flow_log = [
                {"Filter": filter_, "FlowLogIds": flow_log_ids, "MaxResults": max_results, "NextToken": next_token}
        ]
        result[flow_log["flow_logId"]] = {"aws_auto.ec2.flow_log.present": new_flow_log}

        for i, data in enumerate(flow_log.get("", ())):
            sub_flow_log = copy.deepcopy(new_flow_log)

            # TODO check for subresouruces
            sub_flow_log.append({})
            sub_flow_log.append({"name": "TODOs"})
            result[f"TODOs-{i}"] = {"aws_auto.ec2.flow_log.present": sub_flow_log}

    return result


