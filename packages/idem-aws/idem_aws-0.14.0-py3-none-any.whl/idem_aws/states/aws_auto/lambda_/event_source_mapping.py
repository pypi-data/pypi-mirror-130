"""
Autogenerated state module using `pop-create-idem <https://gitlab.com/saltstack/pop/pop-create-idem>`__

hub.exec.boto3.client.lambda_.create_event_source_mapping
hub.exec.boto3.client.lambda_.delete_event_source_mapping
hub.exec.boto3.client.lambda_.get_event_source_mapping
hub.exec.boto3.client.lambda_.list_event_source_mappings
hub.exec.boto3.client.lambda_.update_event_source_mapping
"""



import copy
from typing import *
import dict_tools.differ as differ

async def present(hub, ctx, name: Text, function_name: Text, event_source_arn: Text = None, enabled: bool = None, batch_size: int = None, maximum_batching_window_in_seconds: int = None, parallelization_factor: int = None, starting_position: Text = None, starting_position_timestamp: Text = None, destination_config: Dict = None, maximum_record_age_in_seconds: int = None, bisect_batch_on_function_error: bool = None, maximum_retry_attempts: int = None, tumbling_window_in_seconds: int = None, topics: List = None, queues: List = None, source_access_configurations: List = None, self_managed_event_source: Dict = None, function_response_types: List = None)  -> Dict[str, Any]:
    r'''
    **Autogenerated function**
    
    Creates a mapping between an event source and an Lambda function. Lambda reads items from the event source and
    triggers the function. For details about each event source type, see the following topics. In particular, each
    of the topics describes the required and optional parameters for the specific event source.      Configuring a
    Dynamo DB stream as an event source      Configuring a Kinesis stream as an event source      Configuring an SQS
    queue as an event source      Configuring an MQ broker as an event source      Configuring MSK as an event
    source      Configuring Self-Managed Apache Kafka as an event source    The following error handling options are
    only available for stream sources (DynamoDB and Kinesis):    BisectBatchOnFunctionError - If the function
    returns an error, split the batch in two and retry.    DestinationConfig - Send discarded records to an Amazon
    SQS queue or Amazon SNS topic.    MaximumRecordAgeInSeconds - Discard records older than the specified age. The
    default value is infinite (-1). When set to infinite (-1), failed records are retried until the record expires
    MaximumRetryAttempts - Discard records after the specified number of retries. The default value is infinite
    (-1). When set to infinite (-1), failed records are retried until the record expires.    ParallelizationFactor -
    Process multiple batches from each shard concurrently.

    Args:
        name(Text): A name, ID, or JMES search path to identify the resource.
        event_source_arn(Text, optional): The Amazon Resource Name (ARN) of the event source.    Amazon Kinesis - The ARN of the data
            stream or a stream consumer.    Amazon DynamoDB Streams - The ARN of the stream.    Amazon
            Simple Queue Service - The ARN of the queue.    Amazon Managed Streaming for Apache Kafka - The
            ARN of the cluster. Defaults to None.
        function_name(Text): The name of the Lambda function.  Name formats     Function name - MyFunction.    Function ARN -
            arn:aws:lambda:us-west-2:123456789012:function:MyFunction.    Version or Alias ARN -
            arn:aws:lambda:us-west-2:123456789012:function:MyFunction:PROD.    Partial ARN -
            123456789012:function:MyFunction.   The length constraint applies only to the full ARN. If you
            specify only the function name, it's limited to 64 characters in length.
        enabled(bool, optional): If true, the event source mapping is active. Set to false to pause polling and invocation. Defaults to None.
        batch_size(int, optional): The maximum number of items to retrieve in a single batch.    Amazon Kinesis - Default 100. Max
            10,000.    Amazon DynamoDB Streams - Default 100. Max 1,000.    Amazon Simple Queue Service -
            Default 10. For standard queues the max is 10,000. For FIFO queues the max is 10.    Amazon
            Managed Streaming for Apache Kafka - Default 100. Max 10,000.    Self-Managed Apache Kafka -
            Default 100. Max 10,000. Defaults to None.
        maximum_batching_window_in_seconds(int, optional): (Streams and SQS standard queues) The maximum amount of time to gather records before invoking
            the function, in seconds. Defaults to None.
        parallelization_factor(int, optional): (Streams only) The number of batches to process from each shard concurrently. Defaults to None.
        starting_position(Text, optional): The position in a stream from which to start reading. Required for Amazon Kinesis, Amazon
            DynamoDB, and Amazon MSK Streams sources. AT_TIMESTAMP is only supported for Amazon Kinesis
            streams. Defaults to None.
        starting_position_timestamp(Text, optional): With StartingPosition set to AT_TIMESTAMP, the time from which to start reading. Defaults to None.
        destination_config(Dict, optional): (Streams only) An Amazon SQS queue or Amazon SNS topic destination for discarded records. Defaults to None.
        maximum_record_age_in_seconds(int, optional): (Streams only) Discard records older than the specified age. The default value is infinite (-1). Defaults to None.
        bisect_batch_on_function_error(bool, optional): (Streams only) If the function returns an error, split the batch in two and retry. Defaults to None.
        maximum_retry_attempts(int, optional): (Streams only) Discard records after the specified number of retries. The default value is
            infinite (-1). When set to infinite (-1), failed records will be retried until the record
            expires. Defaults to None.
        tumbling_window_in_seconds(int, optional): (Streams only) The duration in seconds of a processing window. The range is between 1 second up
            to 900 seconds. Defaults to None.
        topics(List, optional): The name of the Kafka topic. Defaults to None.
        queues(List, optional):  (MQ) The name of the Amazon MQ broker destination queue to consume. Defaults to None.
        source_access_configurations(List, optional): An array of authentication protocols or VPC components required to secure your event source. Defaults to None.
        self_managed_event_source(Dict, optional): The Self-Managed Apache Kafka cluster to send records. Defaults to None.
        function_response_types(List, optional): (Streams only) A list of current response type enums applied to the event source mapping. Defaults to None.

    Returns:
        Dict[str, Any]

    Examples:

        .. code-block:: sls

            resource_is_present:
              aws_auto.lambda_.event_source_mapping.present:
                - name: value
                - function_name: value
    '''

    
    result = dict(comment="", changes= None, name=name, result=True)
    ret = await hub.exec.boto3.client.lambda_.event_source_mapping.id(
        ctx,
        jmes_path=name
    )
    if ret["status"]:
        # name is now the first id that matched the JMES search path
        name = ret["ret"]
    

    
    before = await hub.exec.boto3.client.lambda_.get_event_source_mapping(name)

    if before:
        result["comment"] = f"'{name}' already exists"
    else:
        try:
            ret = await hub.exec.boto3.client.lambda_.create_event_source_mapping(
                ctx,
                
                
                **{"EventSourceArn": event_source_arn, "FunctionName": function_name, "Enabled": enabled, "BatchSize": batch_size, "MaximumBatchingWindowInSeconds": maximum_batching_window_in_seconds, "ParallelizationFactor": parallelization_factor, "StartingPosition": starting_position, "StartingPositionTimestamp": starting_position_timestamp, "DestinationConfig": destination_config, "MaximumRecordAgeInSeconds": maximum_record_age_in_seconds, "BisectBatchOnFunctionError": bisect_batch_on_function_error, "MaximumRetryAttempts": maximum_retry_attempts, "TumblingWindowInSeconds": tumbling_window_in_seconds, "Topics": topics, "Queues": queues, "SourceAccessConfigurations": source_access_configurations, "SelfManagedEventSource": self_managed_event_source, "FunctionResponseTypes": function_response_types}
            )
            result["result"] = ret["status"]
            if not result["result"]:
                result["comment"] = ret["comment"]
                return result
            ret["comment"] = f"Created '{name}'"
        except hub.tool.boto3.exception.ClientError as e:
            result["comment"] = f"{e.__class__.__name__}: {e}"

    
    # TODO perform other modifications as needed here
    ...

    after = await hub.exec.boto3.client.lambda_.get_event_source_mapping(name)
    result["changes"] = differ.deep_diff(before, after)
    return result



async def absent(hub, ctx, name: Text, uuid: Text)  -> Dict[str, Any]:
    r'''
    **Autogenerated function**
    
    Deletes an event source mapping. You can get the identifier of a mapping from the output of
    ListEventSourceMappings. When you delete an event source mapping, it enters a Deleting state and might not be
    completely deleted for several seconds.

    Args:
        name(Text): A name, ID, or JMES search path to identify the resource.
        uuid(Text): The identifier of the event source mapping.

    Returns:
        Dict[str, Any]

    Examples:

        .. code-block:: sls

            resource_is_absent:
              aws_auto.lambda_.event_source_mapping.absent:
                - name: value
                - uuid: value
    '''

    
    result = dict(comment="", changes= None, name=name, result=True)
    ret = await hub.exec.boto3.client.lambda_.event_source_mapping.id(
        ctx,
        jmes_path=name
    )
    if ret["status"]:
        # name is now the first id that matched the JMES search path
        name = ret["ret"]
    

    

    before = await hub.exec.boto3.client.lambda_.get_event_source_mapping(name)

    if not before:
        result["comment"] = f"'{name}' already absent"
    else:
        try:
            ret = await hub.exec.boto3.client.lambda_.delete_event_source_mapping(
                ctx,
                
                
                **{"UUID": uuid}
            )
            result["result"] = ret["status"]
            if not result["result"]:
                result["comment"] = ret["comment"]
                return result
            result["comment"] = f"Deleted '{name}'"
        except hub.tool.boto3.exception.ClientError as e:
            result["comment"] = f"{e.__class__.__name__}: {e}"

    

    after = await hub.exec.boto3.client.lambda_.get_event_source_mapping(name)
    result["changes"] = differ.deep_diff(before, after)
    return result



async def describe(hub, ctx)  -> Dict[str, Dict[str, Any]]:
    r'''
    **Autogenerated function**

    Describe the resource in a way that can be recreated/managed with the corresponding "present" function

    
    Lists event source mappings. Specify an EventSourceArn to only show event source mappings for a single event
    source.


    Returns:
        Dict[str, Any]

    Examples:

        .. code-block:: bash

            $ idem describe aws_auto.lambda_.event_source_mapping
    '''

    
    result = {}

    ret = await hub.exec.boto3.client.lambda_.list_event_source_mappings(ctx)
    if not ret["status"]:
        hub.log.debug(f"Could not describe event_source_mapping {ret['comment']}")
        return result

    for event_source_mapping in ret["ret"]["TODO"]:
        new_event_source_mapping = [
                {"EventSourceArn": event_source_arn, "FunctionName": function_name, "Marker": marker, "MaxItems": max_items}
        ]
        result[event_source_mapping["event_source_mappingId"]] = {"aws_auto.lambda_.event_source_mapping.present": new_event_source_mapping}

        for i, data in enumerate(event_source_mapping.get("", ())):
            sub_event_source_mapping = copy.deepcopy(new_event_source_mapping)

            # TODO check for subresouruces
            sub_event_source_mapping.append({})
            sub_event_source_mapping.append({"name": "TODOs"})
            result[f"TODOs-{i}"] = {"aws_auto.lambda_.event_source_mapping.present": sub_event_source_mapping}

    return result


