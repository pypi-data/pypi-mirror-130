"""
Autogenerated state module using `pop-create-idem <https://gitlab.com/saltstack/pop/pop-create-idem>`__

hub.exec.boto3.client.machinelearning.create_evaluation
hub.exec.boto3.client.machinelearning.delete_evaluation
hub.exec.boto3.client.machinelearning.describe_evaluations
hub.exec.boto3.client.machinelearning.get_evaluation
hub.exec.boto3.client.machinelearning.update_evaluation
"""



from typing import *
import dict_tools.differ as differ
async def present(hub, ctx, name: Text, evaluation_id: Text, ml_model_id: Text, evaluation_data_source_id: Text, evaluation_name: Text = None)  -> Dict[str, Any]:
    r'''
    **Autogenerated function**
    
    Creates a new Evaluation of an MLModel. An MLModel is evaluated on a set of observations associated to a
    DataSource. Like a DataSource for an MLModel, the DataSource for an Evaluation contains values for the Target
    Variable. The Evaluation compares the predicted result for each observation to the actual outcome and provides a
    summary so that you know how effective the MLModel functions on the test data. Evaluation generates a relevant
    performance metric, such as BinaryAUC, RegressionRMSE or MulticlassAvgFScore based on the corresponding
    MLModelType: BINARY, REGRESSION or MULTICLASS.   CreateEvaluation is an asynchronous operation. In response to
    CreateEvaluation, Amazon Machine Learning (Amazon ML) immediately returns and sets the evaluation status to
    PENDING. After the Evaluation is created and ready for use, Amazon ML sets the status to COMPLETED.  You can use
    the GetEvaluation operation to check progress of the evaluation during the creation operation.

    Args:
        name(Text): A name, ID, or JMES search path to identify the resource.
        evaluation_id(Text): A user-supplied ID that uniquely identifies the Evaluation.
        evaluation_name(Text, optional): A user-supplied name or description of the Evaluation. Defaults to None.
        ml_model_id(Text): The ID of the MLModel to evaluate. The schema used in creating the MLModel must match the schema
            of the DataSource used in the Evaluation.
        evaluation_data_source_id(Text): The ID of the DataSource for the evaluation. The schema of the DataSource must match the schema
            used to create the MLModel.

    Returns:
        Dict[str, Any]

    Examples:

        .. code-block:: sls

            resource_is_present:
              aws_auto.machinelearning.evaluation.present:
                - name: value
                - evaluation_id: value
                - ml_model_id: value
                - evaluation_data_source_id: value
    '''
    
    result = dict(comment="", changes= None, name=name, result=True)
    ret = await hub.exec.boto3.client.machinelearning.evaluation.id(
        ctx,
        jmes_path=name
    )
    if ret["status"]:
        # name is now the first id that matched the JMES search path
        name = ret["ret"]
    

    
    before = await hub.exec.boto3.client.machinelearning.describe_evaluations(name)

    if before:
        result["comment"] = f"'{name}' already exists"
    else:
        try:
            ret = await hub.exec.boto3.client.machinelearning.create_evaluation(
                ctx,
                
                
                **{"EvaluationId": evaluation_id, "EvaluationName": evaluation_name, "MLModelId": ml_model_id, "EvaluationDataSourceId": evaluation_data_source_id}
            )
            result["result"] = ret["status"]
            if not result["result"]:
                result["comment"] = ret["comment"]
                return result
            ret["comment"] = f"Created '{name}'"
        except hub.tool.boto3.exception.ClientError as e:
            result["comment"] = f"{e.__class__.__name__}: {e}"

    
    # TODO perform other modifications as needed here
    ...

    after = await hub.exec.boto3.client.machinelearning.describe_evaluations(name)
    result["changes"] = differ.deep_diff(before, after)
    return result

async def absent(hub, ctx, name: Text, evaluation_id: Text)  -> Dict[str, Any]:
    r'''
    **Autogenerated function**
    
    Assigns the DELETED status to an Evaluation, rendering it unusable. After invoking the DeleteEvaluation
    operation, you can use the GetEvaluation operation to verify that the status of the Evaluation changed to
    DELETED.  Caution: The results of the DeleteEvaluation operation are irreversible.

    Args:
        name(Text): A name, ID, or JMES search path to identify the resource.
        evaluation_id(Text): A user-supplied ID that uniquely identifies the Evaluation to delete.

    Returns:
        Dict[str, Any]

    Examples:

        .. code-block:: sls

            resource_is_absent:
              aws_auto.machinelearning.evaluation.absent:
                - name: value
                - evaluation_id: value
    '''
    
    result = dict(comment="", changes= None, name=name, result=True)
    ret = await hub.exec.boto3.client.machinelearning.evaluation.id(
        ctx,
        jmes_path=name
    )
    if ret["status"]:
        # name is now the first id that matched the JMES search path
        name = ret["ret"]
    

    

    before = await hub.exec.boto3.client.machinelearning.describe_evaluations(name)

    if not before:
        result["comment"] = f"'{name}' already absent"
    else:
        try:
            ret = await hub.exec.boto3.client.machinelearning.delete_evaluation(
                ctx,
                
                
                **{"EvaluationId": evaluation_id}
            )
            result["result"] = ret["status"]
            if not result["result"]:
                result["comment"] = ret["comment"]
                return result
            result["comment"] = f"Deleted '{name}'"
        except hub.tool.boto3.exception.ClientError as e:
            result["comment"] = f"{e.__class__.__name__}: {e}"

    

    after = await hub.exec.boto3.client.machinelearning.describe_evaluations(name)
    result["changes"] = differ.deep_diff(before, after)
    return result

