"""
Autogenerated state module using `pop-create-idem <https://gitlab.com/saltstack/pop/pop-create-idem>`__

hub.exec.boto3.client.forecast.create_dataset_import_job
hub.exec.boto3.client.forecast.delete_dataset_import_job
hub.exec.boto3.client.forecast.describe_dataset_import_job
hub.exec.boto3.client.forecast.list_dataset_import_jobs
"""



from typing import *
import dict_tools.differ as differ
async def present(hub, ctx, name: Text, dataset_import_job_name: Text, dataset_arn: Text, data_source: Dict, timestamp_format: Text = None, time_zone: Text = None, use_geolocation_for_time_zone: bool = None, geolocation_format: Text = None, tags: List = None)  -> Dict[str, Any]:
    r'''
    **Autogenerated function**
    
    Imports your training data to an Amazon Forecast dataset. You provide the location of your training data in an
    Amazon Simple Storage Service (Amazon S3) bucket and the Amazon Resource Name (ARN) of the dataset that you want
    to import the data to. You must specify a DataSource object that includes an AWS Identity and Access Management
    (IAM) role that Amazon Forecast can assume to access the data, as Amazon Forecast makes a copy of your data and
    processes it in an internal AWS system. For more information, see aws-forecast-iam-roles. The training data must
    be in CSV format. The delimiter must be a comma (,). You can specify the path to a specific CSV file, the S3
    bucket, or to a folder in the S3 bucket. For the latter two cases, Amazon Forecast imports all files up to the
    limit of 10,000 files. Because dataset imports are not aggregated, your most recent dataset import is the one
    that is used when training a predictor or generating a forecast. Make sure that your most recent dataset import
    contains all of the data you want to model off of, and not just the new data collected since the previous
    import. To get a list of all your dataset import jobs, filtered by specified criteria, use the
    ListDatasetImportJobs operation.

    Args:
        name(Text): A name, ID, or JMES search path to identify the resource.
        dataset_import_job_name(Text): The name for the dataset import job. We recommend including the current timestamp in the name,
            for example, 20190721DatasetImport. This can help you avoid getting a
            ResourceAlreadyExistsException exception.
        dataset_arn(Text): The Amazon Resource Name (ARN) of the Amazon Forecast dataset that you want to import data to.
        data_source(Dict): The location of the training data to import and an AWS Identity and Access Management (IAM) role
            that Amazon Forecast can assume to access the data. The training data must be stored in an
            Amazon S3 bucket. If encryption is used, DataSource must include an AWS Key Management Service
            (KMS) key and the IAM role must allow Amazon Forecast permission to access the key. The KMS key
            and IAM role must match those specified in the EncryptionConfig parameter of the CreateDataset
            operation.
        timestamp_format(Text, optional): The format of timestamps in the dataset. The format that you specify depends on the
            DataFrequency specified when the dataset was created. The following formats are supported
            "yyyy-MM-dd" For the following data frequencies: Y, M, W, and D   "yyyy-MM-dd HH:mm:ss" For the
            following data frequencies: H, 30min, 15min, and 1min; and optionally, for: Y, M, W, and D   If
            the format isn't specified, Amazon Forecast expects the format to be "yyyy-MM-dd HH:mm:ss". Defaults to None.
        time_zone(Text, optional): A single time zone for every item in your dataset. This option is ideal for datasets with all
            timestamps within a single time zone, or if all timestamps are normalized to a single time zone.
            Refer to the Joda-Time API for a complete list of valid time zone names. Defaults to None.
        use_geolocation_for_time_zone(bool, optional): Automatically derive time zone information from the geolocation attribute. This option is ideal
            for datasets that contain timestamps in multiple time zones and those timestamps are expressed
            in local time. Defaults to None.
        geolocation_format(Text, optional): The format of the geolocation attribute. The geolocation attribute can be formatted in one of
            two ways:    LAT_LONG - the latitude and longitude in decimal format (Example: 47.61_-122.33).
            CC_POSTALCODE (US Only) - the country code (US), followed by the 5-digit ZIP code (Example:
            US_98121). Defaults to None.
        tags(List, optional): The optional metadata that you apply to the dataset import job to help you categorize and
            organize them. Each tag consists of a key and an optional value, both of which you define. The
            following basic restrictions apply to tags:   Maximum number of tags per resource - 50.   For
            each resource, each tag key must be unique, and each tag key can have only one value.   Maximum
            key length - 128 Unicode characters in UTF-8.   Maximum value length - 256 Unicode characters in
            UTF-8.   If your tagging schema is used across multiple services and resources, remember that
            other services may have restrictions on allowed characters. Generally allowed characters are:
            letters, numbers, and spaces representable in UTF-8, and the following characters: + - = . _ : /
            @.   Tag keys and values are case sensitive.   Do not use aws:, AWS:, or any upper or lowercase
            combination of such as a prefix for keys as it is reserved for AWS use. You cannot edit or
            delete tag keys with this prefix. Values can have this prefix. If a tag value has aws as its
            prefix but the key does not, then Forecast considers it to be a user tag and will count against
            the limit of 50 tags. Tags with only the key prefix of aws do not count against your tags per
            resource limit. Defaults to None.

    Returns:
        Dict[str, Any]

    Examples:

        .. code-block:: sls

            resource_is_present:
              aws_auto.forecast.dataset_import_job.present:
                - name: value
                - dataset_import_job_name: value
                - dataset_arn: value
                - data_source: value
    '''
    
    result = dict(comment="", changes= None, name=name, result=True)
    ret = await hub.exec.boto3.client.forecast.dataset_import_job.id(
        ctx,
        jmes_path=name
    )
    if ret["status"]:
        # name is now the first id that matched the JMES search path
        name = ret["ret"]
    

    
    before = await hub.exec.boto3.client.forecast.describe_dataset_import_job(name)

    if before:
        result["comment"] = f"'{name}' already exists"
    else:
        try:
            ret = await hub.exec.boto3.client.forecast.create_dataset_import_job(
                ctx,
                
                
                **{"DatasetImportJobName": dataset_import_job_name, "DatasetArn": dataset_arn, "DataSource": data_source, "TimestampFormat": timestamp_format, "TimeZone": time_zone, "UseGeolocationForTimeZone": use_geolocation_for_time_zone, "GeolocationFormat": geolocation_format, "Tags": tags}
            )
            result["result"] = ret["status"]
            if not result["result"]:
                result["comment"] = ret["comment"]
                return result
            ret["comment"] = f"Created '{name}'"
        except hub.tool.boto3.exception.ClientError as e:
            result["comment"] = f"{e.__class__.__name__}: {e}"

    
    # TODO perform other modifications as needed here
    ...

    after = await hub.exec.boto3.client.forecast.describe_dataset_import_job(name)
    result["changes"] = differ.deep_diff(before, after)
    return result

async def absent(hub, ctx, name: Text, dataset_import_job_arn: Text)  -> Dict[str, Any]:
    r'''
    **Autogenerated function**
    
    Deletes a dataset import job created using the CreateDatasetImportJob operation. You can delete only dataset
    import jobs that have a status of ACTIVE or CREATE_FAILED. To get the status, use the DescribeDatasetImportJob
    operation.

    Args:
        name(Text): A name, ID, or JMES search path to identify the resource.
        dataset_import_job_arn(Text): The Amazon Resource Name (ARN) of the dataset import job to delete.

    Returns:
        Dict[str, Any]

    Examples:

        .. code-block:: sls

            resource_is_absent:
              aws_auto.forecast.dataset_import_job.absent:
                - name: value
                - dataset_import_job_arn: value
    '''
    
    result = dict(comment="", changes= None, name=name, result=True)
    ret = await hub.exec.boto3.client.forecast.dataset_import_job.id(
        ctx,
        jmes_path=name
    )
    if ret["status"]:
        # name is now the first id that matched the JMES search path
        name = ret["ret"]
    

    

    before = await hub.exec.boto3.client.forecast.describe_dataset_import_job(name)

    if not before:
        result["comment"] = f"'{name}' already absent"
    else:
        try:
            ret = await hub.exec.boto3.client.forecast.delete_dataset_import_job(
                ctx,
                
                
                **{"DatasetImportJobArn": dataset_import_job_arn}
            )
            result["result"] = ret["status"]
            if not result["result"]:
                result["comment"] = ret["comment"]
                return result
            result["comment"] = f"Deleted '{name}'"
        except hub.tool.boto3.exception.ClientError as e:
            result["comment"] = f"{e.__class__.__name__}: {e}"

    

    after = await hub.exec.boto3.client.forecast.describe_dataset_import_job(name)
    result["changes"] = differ.deep_diff(before, after)
    return result

