{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ref: This Notebook comes from [HuggingFace Examples ðŸ¤—](https://librecv.github.io/blog/gans/pytorch/2021/02/13/Pix2Pix-explained-with-code.html)\n",
    "\n",
    "\n",
    "<!--<badge>--><a href=\"https://colab.research.google.com/github/gradsflow/gradsflow/blob/main/examples/nbs/2021-10-3-huggingface-training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><!--</badge>-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k8qR0SXcno2g"
   },
   "outputs": [],
   "source": [
    "# installation\n",
    "# !pip install git+https://github.com/gradsflow/gradsflow@main -q -U\n",
    "# !pip install -U transformers datasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LcYxAsGEno2q",
    "outputId": "7a81a66b-f13c-4dcc-bd6f-4039ceca3584"
   },
   "outputs": [],
   "source": [
    "# ! wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "# ! tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ck8msznLno2r"
   },
   "source": [
    "This data is organized into `pos` and `neg` folders with one text file per example. Let's write a function that can\n",
    "read this in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hxUTeD3Ano2r"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def read_imdb_split(split_dir):\n",
    "    split_dir = Path(split_dir)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for label_dir in [\"pos\", \"neg\"]:\n",
    "        for text_file in (split_dir / label_dir).iterdir():\n",
    "            texts.append(text_file.read_text())\n",
    "            labels.append(0 if label_dir is \"neg\" else 1)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "train_texts, train_labels = read_imdb_split(\"aclImdb/train\")\n",
    "test_texts, test_labels = read_imdb_split(\"aclImdb/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Srd0eFF8no2s"
   },
   "source": [
    "We now have a train and test dataset, but let's also also create a validation set which we can use for for evaluation\n",
    "and tuning without tainting our test set results. Sklearn has a convenient utility for creating such splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0LHXa07Eno2s"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1oPprUhno2s"
   },
   "source": [
    "Alright, we've read in our dataset. Now let's tackle tokenization. We'll eventually train a classifier using\n",
    "pre-trained DistilBert, so let's use the DistilBert tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "bcd76d72c9bf4a8fa429a0d9c08cfe8f",
      "6fdb1152ec2a45f5a1ed9fd39dd4e11c",
      "bdd737cb74a945809965ed56b6a9a587",
      "8ae96823fe664e8187b7ee32b244d245",
      "3daa1fc8c80c49728995605640fea148",
      "5469d92833564c858db09b41c2031a32",
      "0259b7f9462144b8b97464debec252fe",
      "2d49c6c32cb94873a3b6dd273cbcf87a",
      "0ea6eee1f7284f89845db95f12fc5837",
      "beedfa113e614755afd803f37b341e0e",
      "a3b92677488b4cf6a3052445bfafed6a",
      "a01ddd6e89bf4eba9a03fbe77b887fc7",
      "ac8f2a26b48a416f9b868d34f5d60f6c",
      "a0a267e4940b4d9e9a73ef95d5b92369",
      "f0b971425fed42bf866619509dde5cec",
      "7b6613e4155548368e0bb43c4be9c312",
      "aedfe33834364a4d974b75729cfacbb2",
      "c348e873aa9a4e95a09e1f66d967244a",
      "e18c348fa07a4ed397f96afc50e0cb7e",
      "7f3ccc7ed50e4515950181f518eba85f",
      "48874d4604b743f9b1d27e8bf397400b",
      "7cf50abfad6e4d99b40456af49d89604",
      "746b4c3eaf7d4f04807a2000e81174ad",
      "7821cb7be63c43e88edaee0d4a4f9f59",
      "6d04e881759f4649a8bab33d02f3c8d5",
      "d8e2e0e4924d4384aa5efb782e63d67c",
      "73898219b63945e8901548248e446062",
      "83fafadc9fd74896952910f2c8a641a4",
      "ff6b50fbd48c4ccbab2d75d8f42c3091",
      "836a59919c8c40c8ab0a935f71dc51aa",
      "4135aec63cf84838999229a5744fc966",
      "11a5b7e2cb28464eaf32a6befb658946",
      "69eb6faac89043b2994d03cb08481952",
      "6ecf5ecf506443e094b38e9367088b54",
      "e983c75794194a94ace2dc82f39643dd",
      "9f7d7d9a7c064512aec6a63207689060",
      "a143fabfcbdc4a64ae86293851da8d5c",
      "2746a3f9e5ef4f7788df950d7923ce81",
      "d3c5d98347bc4354a8fb878e7717fce9",
      "4a331a73a61248c2b518e5c8e4e4c4cf",
      "04a10010ea6e4d7f8d8602b434721d47",
      "5c411a67af414b75803072404fe6f097",
      "aca9bc8848f24ca2a97d3dbbdc465b77",
      "6d1f2bf975a741b295c1df2fa4745303"
     ]
    },
    "id": "KBwynpwTno2t",
    "outputId": "dfc1421b-20b4-46ae-a739-77638393e1ca"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9Z7EQ-Dno2t"
   },
   "source": [
    "Now we can simply pass our texts to the tokenizer. We'll pass `truncation=True` and `padding=True`, which will\n",
    "ensure that all of our sequences are padded to the same length and are truncated to be no longer model's maximum input\n",
    "length. This will allow us to feed batches of sequences into the model at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nLJFJAK1no2t"
   },
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16esQPYtno2u"
   },
   "source": [
    "Now, let's turn our labels and encodings into a Dataset object. In PyTorch, this is done by subclassing a\n",
    "`torch.utils.data.Dataset` object and implementing `__len__` and `__getitem__`. In TensorFlow, we pass our input\n",
    "encodings and labels to the `from_tensor_slices` constructor method. We put the data in this format so that the data\n",
    "can be easily batched such that each key in the batch encoding corresponds to a named parameter of the\n",
    "`DistilBertForSequenceClassification.forward` method of the model we will train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3aPnVypsno2u"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "val_dataset = IMDbDataset(val_encodings, val_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omNbyULvno2u"
   },
   "source": [
    "Now that our datasets our ready, we can fine-tune a model either with the ðŸ¤—\n",
    "`Trainer`/`TFTrainer` or with native PyTorch/TensorFlow. See [training](https://huggingface.co/transformers/training.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154,
     "referenced_widgets": [
      "bf0c27c7ca484584955f0900f0a0030b",
      "2525a894416f4e14b1ef3af079b0c1df",
      "380d3e655f1041bb87c2d75680539f9f",
      "7a6fddf7759245319004a09943b959af",
      "d27f0f2392e846d8b968d6ab5bf80e10",
      "2339fe4df270499a9382badbc3aab795",
      "62273ac6acd045d8b23ac93349262e83",
      "706a33dcde6c449cbc93147357305963",
      "7cc1ebecd5f240fe80cf82e9ed978e61",
      "a8d23a1fef6c4acda8414e5062222e2b",
      "94a42299ddc64c4f962c2c4458c77092"
     ]
    },
    "id": "yVd6T259no2w",
    "outputId": "b19d6163-ebac-4042-bb2f-95c8a6d11f9d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertForSequenceClassification, AdamW\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_xOSeuEoTSg"
   },
   "outputs": [],
   "source": [
    "from gradsflow import Model, AutoDataset\n",
    "\n",
    "\n",
    "class GFModel(Model):\n",
    "    def __init__(self, learner):\n",
    "        super().__init__(learner, accelerator_config={\"fp16\": True})\n",
    "\n",
    "    def compile(self, metrics):\n",
    "        optimizer = AdamW(self.learner.parameters(), lr=5e-5)\n",
    "        self.optimizer = self.prepare_optimizer(optimizer)\n",
    "        super().compile(metrics=metrics)\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        outputs = self.learner(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        self.backward(loss)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.tracker.track(\"train/step_loss\", loss, render=True)\n",
    "        return {\"loss\": loss, \"logits\": outputs[1].cpu(), \"target\": labels.cpu()}\n",
    "\n",
    "    def val_step(self, batch):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        outputs = self.learner(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        self.tracker.track(\"val/step_loss\", loss, render=True)\n",
    "        return {\"loss\": loss, \"logits\": outputs[1].cpu(), \"target\": labels.cpu()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qKCP6NxHoTQH",
    "outputId": "f0a18809-2fdb-4b79-a61c-e80a834b9717"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = AutoDataset(train_dataloader=train_loader, val_dataloader=val_loader)\n",
    "\n",
    "gf_model = GFModel(\n",
    "    model,\n",
    ")\n",
    "gf_model.compile(metrics=\"accuracy\")\n",
    "\n",
    "gf_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f5pUYMV0oTJ0",
    "outputId": "75a10fe2-6443-4550-99c4-8d5ade812bc2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345f1c9109dc425aa3926f0f9e902c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Tracker(max_epochs=0, current_epoch=0, current_step=1094, steps_per_epoch=None, train=TrackingValues(loss=0.2676990067182042, steps=1094, step_loss=0.28964027762413025, metrics={'Accuracy': tensor(0.8925)}), val=TrackingValues(loss=0.20485645957120358, steps=469, step_loss=tensor(0.1323, device='cuda:0'), metrics={'Accuracy': tensor(0.9188)}))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gf_model.fit(data, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrackingValues(loss=0.2676990067182042, steps=1094, step_loss=0.28964027762413025, metrics={'Accuracy': tensor(0.8925)})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gf_model.tracker.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrackingValues(loss=0.20485645957120358, steps=469, step_loss=tensor(0.1323, device='cuda:0'), metrics={'Accuracy': tensor(0.9188)})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gf_model.tracker.val"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "custom_datasets.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
