"""This module implements Dirichlet-distributed random vectors and arrays,
allowing only non-Bayesian ML point estimation of the concentration parameters of the Dirichlet density.

A Dirichlet-distributed vector is a random vector of fractions,
with sum == 1., and all elements in (0., 1.).

The Dirichlet-distributed vector may be used as a probability-mass parameter
for a corresponding Multinomial-distributed count array of integer elements.

*** Main Classes:
DirichletVector: model for Dirichlet-distributed probability-mass vector,
    with all elements in (0., 1.), and sum of all elements == 1.
    The shape is arbitrary, but the probability-mass is treated as a 1D vector.
    The distribution is specified by an array of concentration parameters.
    The concentration parameters can be adapted to
    (1) a sequence of observed probability-mass vectors that may be samples of the DirichletVector, or
    (2) a sequence of multinomial integer arrays that may be generated from a
        MultinomialVector object with probability-mass randomly drawn from the DirichletVector object.

*** Module functions:
log_multi_beta = log normalization factor for Dirichlet distribution
d_log_multi_beta = gradient of log_multi_beta

*** Version History:
2019-12-22, first tested version including Multinomial
2019-12-29, new ConcVector.initialize to be reasonably independent of array size
2020-01-04, this separate module only for Dirichlet-distribution class
2020-01-08, simplified to DirichletVector allowing only ROW vector, no aggregration
"""
# *** should clean up for unused old code ****
import numpy as np

from scipy.special import gammaln, psi
# from scipy.optimize import minimize

from scipy.optimize import brentq

import logging

# ------------------------------------------------------------------------
logger = logging.getLogger(__name__)
# logger.setLevel(logging.DEBUG)  # test

JEFFREYS_CONC = 0.5

# *** This old part only for ConcVector, not needed for EmaCalc
# PRIOR_CONC_MODE = 0.5
# PRIOR_CONC_MEAN = 10.  # seems OK tested 2019-12-29 with K=5
# # = prior mode and mean for concentration parameters of Dirichlet mixture components
# # with artificial profile size K=PRIOR_CONC_K
# PRIOR_CONC_K = 5
# # Actual mode and mean calculated by _equivalent_conc, for given array size.
#
# # PRIOR_CONC_MEAN is the main control of mixture sparsity behavior,
# # larger values -> more concentrated clusters, allowing fewer members.

# RNG = np.random.default_rng()
# = default module-global random generator
# may be changed for module, or for each random class instance
# ***** safer to have a separate generator in each RV instance in case of multi-processing


# # ----------------------------------------------------------
# class ConcVector:  # ******* not needed for EmaCalc ******************
#     """Implementing prior distribution of concentration parameters for use by DirichletVector,
#     when learning from observed count profiles assumed generated by a MultinomialVector object
#     with the DirichletVector instance as prob property.
#
#     This prior is a fixed gamma distribution with all elements independent,
#     specified by scalar hyper-parameters alpha, beta for the gamma distribution.
#     """
#     def __init__(self, alpha, beta):
#         """
#         :param alpha: scalar shape parameter of gamma density, > 0
#         :param beta: scalar inverse-scale parameter of gamma density, > 0
#         """
#         self.alpha = alpha
#         self.beta = beta
#
#     @classmethod
#     def initialize(cls, size):
#         """Convenience initialization
#         :param size: size of DirichletVector for which this prior is used
#         :return: a cls instance
#         """
#         mode = _equivalent_conc(size, PRIOR_CONC_MODE, PRIOR_CONC_K)
#         mean = _equivalent_conc(size, PRIOR_CONC_MEAN, PRIOR_CONC_K)
#         # (alpha - 1) / beta = mode of gamma distribution
#         # alpha / beta = mean of gamma distribution
#         # beta * mean - beta * mode = 1
#         beta = 1. / (mean - mode)
#         alpha = mean * beta
#         return cls(alpha, beta)
#
#     def logprob(self, a):
#         """log p(a)
#         calculated with independent UN-normalized gamma distributions,
#         because self properties will remain constant during use.
#         :param a: array with tentative concentration parameters for a DirichletVector object.
#         :return: scalar lp
#         """
#         lp = (self.alpha - 1.) * np.log(a) - self.beta * a
#         return np.sum(lp)  # ******* sum across all dimensions in arbitrary shape
#
#     def d_logprob(self, a):
#         """Gradient of logprob(a)
#         :param a: array with tentative concentration parameters
#         :return: scalar dlp
#             dlp[...] = d log p(a) / d a[...]
#             dlp.shape == a.shape
#          """
#         dlp = (self.alpha - 1.) / a - self.beta
#         return dlp
#
#
# # DEFAULT_CONC_PRIOR = ConcVector.initialize(mode=PRIOR_CONC_MODE, mean=PRIOR_CONC_MEAN)
# # DEFAULT_CONC_PRIOR = ConcVector.initialize(size=PRIOR_CONC_K)
# # to be used in case a specific prior is not defined by user


# --------------------------------------------------------------------------------
class DirichletVector:  # *********** simplify code to only ROW vector **********
    """Representing a multivariate Dirichlet-distributed array.
    The array may have any shape,
    but the distribution is calculated just as
    if the array is a one-dimensional vector.
    Thus, the sum of all array elements must be unity.

    If U is a Dirichlet Random Vector, with elements U_i,
    the probability density function of U is determined by
    a vector of Concentration Parameters alpha, as
    p_U(u) = (1 / B(alpha) ) * prod_i u_i**(alpha_i - 1),
    B(alpha) = ( prod_i Gamma( alpha_i) ) / Gamma( sum_i alpha_i ),
    within the support domain
    0 < u_i < 1, all i, and sum_i( u_i ) == 1
    The normalization factor B(alpha) is implemented by module function
    log_multi_beta.
    """
    def __init__(self, alpha, rng=None):
        """
        :param alpha: array-like sequence of concentration parameters
        :param rng: (optional) numpy.random.Generator object
        """
        self.alpha = np.array(alpha, dtype=float)
        if rng is None:
            self.rng = np.random.default_rng()
        else:
            self.rng = rng

    def __repr__(self):
        skip = '\n\t'
        return (self.__class__.__name__ + '(' + skip +
                (',' + skip).join(f'{key}={repr(v)}'
                                  for (key, v) in vars(self).items()) +
                skip + ')')

    # @classmethod
    # def initialize(cls, x, **kwargs):  # **********************
    #     """Crude initial setting of concentration parameters
    #     :param x: array-like 1D list with non-normalized row vector(s)
    #         that might be generated from a cls instance,
    #         OR from a multinomial distribution with a cls instance as probability
    #         x[..., :] = ...-th row vector
    #     :param kwargs: (optional) any other key-word arguments for __init__
    #     :return: a new cls instance
    #     """
    #     a = np.array(x) + JEFFREYS_CONC
    #     # including Jeffreys concentration as pseudo-count
    #     a /= np.sum(a, axis=-1, keepdims=True)
    #     a *= JEFFREYS_CONC * a.shape[-1]
    #     # = normalized with average conc = JEFFREYS_CONC
    #     return cls(a, **kwargs)

    @property
    def shape(self):
        return self.alpha.shape

    @property
    def size(self):
        return self.alpha.size

    @property
    def ndim(self):
        """Dimensionality of self"""
        return self.alpha.ndim

    @property
    def mean(self):
        return self.alpha / np.sum(self.alpha)

    @property
    def var(self):
        """Dirichlet variance Ref Bishop Eq B.18
        :return: v = array with
            v[...] = variance of ...-th element random array
            v.shape == self.shape
        """
        a = self.alpha
        a0 = np.sum(a)  # across all elements regardless of shape
        return a * (a0 - a) / (a0**2 * (a0 + 1))

    @property
    def cov(self):
        """Ref Leijon PattRecKomp, Ch. 8
        Bishop Eq B.19 only for off-diag elements
        :return: c = 2D or multi-dim array with
            c[...] = 2D square covariance matrix for ...-th random vector, i.e.
            c[..., j, k] = covariance between (j,k)-th elements of ...-th random vector
            c.shape == (*self.shape[:-1], self.shape[-1], self.shape[-1])
        Note: can handle higher-dim self, but returns 2D square cov matrix of flattened self.
        """
        return _cov_dirichlet(self.alpha.reshape(-1))

    @property
    def mean_log(self):
        """E{ log U } where U is DirichletVector self
        :return: ml = array with
            ml[..., k] = E{ log self[..., k] }
            ml.shape == self.shape
        """
        return psi(self.alpha) - psi(np.sum(self.alpha))

    @property
    def mean_root(self):
        """E{ sqrt(U) } element-wise, where U is DirichletVector self
        :return: ml = array with
            r[..., k] = E{ sqrt(self[..., k]) }
            r.shape == self.shape
        """
        a0 = np.sum(self.alpha)  # , keepdims=True)
        ln_r = (gammaln(a0) - gammaln(a0 + 0.5)
                + gammaln(self.alpha + 0.5) - gammaln(self.alpha))
        return np.exp(ln_r)

    def mean_square_hellinger(self, othr):
        """Expected square Hellinger distance between self and othr,
        assuming independence
        :param othr: object with same class and shape as self
        :return: scalar h2 = E{H^2(self, othr)} = 1 - sum_k E{sqrt( u_k v_k ) }
            where u = (..., u_k, ...) = self, and v = (..., v_k, ...) = othr
        """
        return 1 - np.dot(self.mean_root, othr.mean_root)

    def relative_entropy(self, othr):
        """Relative Entropy = Kullback-Leibler divergence
        between two DirichletVector instances.
        :param othr: DirichletVector instance with same shape as self
        :return: KLdiv = scalar
            KLdiv = E{ log p_self(U) / p_othr(U) } evaluated for U = self
        """
        if self.shape == othr.shape:
            return (gammaln(np.sum(self.alpha)) - gammaln(np.sum(othr.alpha))
                    + np.sum(gammaln(othr.alpha) - gammaln(self.alpha))
                    + np.dot((self.alpha - othr.alpha).reshape(-1),
                             self.mean_log.reshape(-1))
                    )
        else:
            logger.warning('relative_entropy: Shape mismatch')
            return np.inf

    def logpdf(self, x):  # ******* arbitrary shape not tested  ******** needed ?
        """log probability density of input sample vector(s).
        :param x: array-like (sequence of) arrays
            that might be samples drawn from self
        :return: lp = scalar or array with one value for each input sample array
            lp.shape == x.shape[:-self.ndim]
        """
        x = np.asarray(x)
        if x.ndim < self.ndim:
            raise RuntimeError('Dimension mismatch')
        if x.shape[-self.ndim:] != self.shape:
            raise RuntimeError('Shape mismatch')
        # x and self.alpha are broadcast-compatible
        sum_axes = tuple(range(-self.ndim, 0))
        ok = np.logical_and(np.all(np.logical_and(0. < x, x < 1.),  # ********* need only 0 < x ************
                                   axis=sum_axes),
                            np.isclose(1., np.sum(x, axis=sum_axes)))
        if not np.all(ok):
            logger.warning('Some input is not Dirichlet -> logpdf = -inf')
        lp = (np.sum((self.alpha - 1.) * np.log(x), axis=sum_axes)
              - log_multi_beta(self.alpha))  # ******* axis=sum_axes ******************'
        if np.isscalar(lp):
            if not ok:
                lp = -np.inf
        else:
            lp[np.logical_not(ok)] = -np.inf
        return lp

    def rvs(self, size=None):
        """Random sample array(s) drawn from self.
        :param size: (optional) scalar integer or tuple of ints with desired number of samples
        :return: u = array of random-generated probability-profile vectors
            Using numpy.random.Generation conventions:
            if size is None:
                u.shape == self.shape
            else:
                u.shape == (*size, *self.shape)
        """
        u = self.rng.dirichlet(alpha=self.alpha.reshape(-1), size=size)
        if size is None:
            return u.reshape(self.shape)
        elif np.iterable(size):
            return u.reshape((*size, *self.shape))
        else:
            return u.reshape((size, *self.shape))

    # def adapt(self, u, w=None):  # *********************
    #     """Adapt concentration parameters to given set of probability-mass data
    #     :param u: sequence of array-like probability vectors assumed drawn from self
    #     :param w: (optional) 1D sequence of weight values for observed data
    #         len(w) == len(x)
    #     :param prior: (optional) prior distribution of concentration parameters **********?
    #     :return: None
    #     Result: self.alpha adapted to data and prior
    #     Method: find MAP point estimate of concentration parameters
    #     """
    #     raise NotImplementedError

    # def adapt2multinomial(self, x, w=None, prior=None):
    #     """Adapt concentration parameters to given set of multinomial data,
    #     for use mainly when self is probability-mass parameter of a MultnomialArray object.
    #     :param x: 2D-array-like sequence of integer data vectors assumed to be
    #         generated from random probability arrays drawn from self,
    #         i.e., following a Dirichlet-Multinomial distribution, given concentration self.alpha
    #         x.shape[1:] == self.shape
    #     :param w: (optional) 1D sequence of weight values for observed data
    #         len(w) == len(x)
    #     :param prior: (optional) prior distribution of concentration parameters
    #     :return: None
    #     Result: self.alpha adapted to data and prior
    #     Method: find MAP point estimate of concentration parameters
    #     """
    #     #  ***** keep it! used by MultinomialVector
    #     x = np.array(x).reshape((len(x), -1))
    #     # = 2D array with flattened row vectors
    #     if w is None:
    #         w = np.ones(len(x))
    #     if prior is None:
    #         prior = ConcVector.initialize(size=self.size)
    #     n_resp = self.alpha.size
    #     bounds = [(0.000001, None) for _ in range(n_resp)]
    #     # to prevent inf log-likelihood
    #     res = minimize(fun=_neg_logprob_conc,
    #                    jac=_d_neg_logprob_conc,
    #                    args=(x, w, prior),
    #                    bounds=bounds,
    #                    x0=self.alpha.reshape(-1))
    #     if res.success:
    #         self.alpha = res.x.reshape(self.shape)
    #         return - res.fun
    #     else:
    #         logger.error(f'DirichletVector.adapt2multinomial minimize res= {res}')
    #         raise RuntimeError('MAP search did not converge. *** Should not happen ***')

    # def copy(self):
    #     """Clone self, in case alpha needs to be changed
    #     :return: a copy of self, except with same rng
    #     """
    #     return self.__class__(alpha=self.alpha.copy(), rng=self.rng)

    # @staticmethod
    # def log_normalization(a):
    #     """Log normalization factor for a single Dirichlet distribution
    #     = log_partition_function = log multinomial_Beta function
    #     :param a: array of concentration parameters
    #     :return: l = scalar
    #     """
    #     return np.sum(gammaln(a)) - gammaln(np.sum(a))
    #
    # @staticmethod
    # def d_log_normalization(a):
    #     """Gradient of log_normalization
    #     :param a: array of concentration parameter vectors stored row-wise
    #     :return: d_log_norm[...] = d log_normalization(a[...]) / d a[...]
    #         log_norm.shape == a.shape
    #     """
    #     return psi(a) - psi(np.sum(a))


# ----------------------------------------------------------------
# class MultinomialVector:
#     """Random vector of integer-valued counts, specified by property
#     prob = a corresponding DirichletVector instance,
#     specified by a vector of concentration parameters.
#     """
#     def __init__(self, prob, rng=None):
#         self.prob = prob
#         if rng is None:
#             self.rng = self.prob.rng
#         else:
#             self.rng = rng
#
#     @classmethod
#     def initialize(cls, shape, rng=None):
#         """Crude initialization, only to set desired shape
#         :param shape: integer or tuple of integers with desired shape
#         :param rng: (optional) numpy.random.Generator object
#         :return: single cls instance
#         """
#         conc = np.full(shape, fill_value=PRIOR_CONC_MODE)
#         return cls(prob=DirichletVector(alpha=conc, rng=rng))
#
#     def select_dim(self, axes):
#         """Create a new object like self, but including only selected axes of self.prob,
#         by aggregation across all other non-selected probability axes.
#         NOTE: Aggregation in a Dirichlet-distributed array defines a new Dirichlet distribution,
#         with summed concentration parameters.
#         :param axes: int or tuple of ints
#         :return: new object defined by reshaped and aggregated prob array
#         """
#         return self.__class__(self.prob.select_dim(axes))
#
#     @property
#     def shape(self):
#         return self.prob.shape
#
#     @property
#     def size(self):
#         return self.prob.size
#
#     @property
#     def ndim(self):
#         return self.prob.ndim
#
#     def log_likelihood(self, x):  # ******* handle arbitrary x shape
#         """log P( x | self ), marginalized across random self.prob
#         i.e., as x generated directly from Dirichlet-Multinomial distribution,
#         NOT INCLUDING the Multinomial normalization factor
#         :param x: single array or list of arrays that might be generated from self
#         :return: lp = array with
#             lp[...] = log P(x[...] | self)
#             lp.shape == x.shape[:-self.ndim]
#             lp.ndim == 0 or 1
#         """
#         x = np.asarray(x)
#         if x.ndim > self.ndim:
#             x1 = x.reshape((len(x), -1))
#         else:
#             x1 = x.reshape(-1)
#         if x1.shape[-1] != self.size:
#             raise RuntimeError('Size Mismatch')
#         # x is broadcast-compatible with self.prob.alpha
#         a = self.prob.alpha.reshape(-1)
#         lp = log_multi_beta(x1 + a) - log_multi_beta(a)
#         # lp.shape = x.shape[:-1], already summed over axis = -1
#         return lp
#
#     def logprob(self, x):
#         """log P( x | self ), marginalized across all self.prob random variables
#         i.e., as x generated directly from Dirichlet-Multinomial distribution,
#         INCLUDING the Multinomial normalization factor
#         :param x: array-like list of vectors that might be generated from self
#         :return: lp = array with
#             lp[...] = log P(x[...] | self)
#             lp.shape == x.shape[:-self.ndim]
#         """
#         x = np.asarray(x).reshape((len(x), -1))
#         # l_norm = np.sum(log_norm_multinomial(x), axis=tuple(range(1 - self.ndim, 0)))
#         return self.log_likelihood(x) - log_norm_multinomial(x)
#
#     def adapt(self, x, w=None, prior=None):
#         """Adapt concentration parameters of self.prob to given set of count data vectors
#         :param x: 2D-array-like sequence integer data vectors assumed to be
#             generated from random probability vectors drawn from self,
#             i.e., following a Dirichlet-Multinomial distribution, given self.alpha.
#             x.shape[-1] == len(self.alpha)
#         :param w: (optional) 1D sequence of weight values for observed data
#             len(w) == len(x)
#         :param prior: (optional) prior distribution of concentration parameters
#         :return: None
#         Result: self.prob.alpha adapted to data and prior
#         Method: find MAP point estimate of concentration parameters
#         """
#         return self.prob.adapt2multinomial(x, w, prior)
#
#     # def adapt(self, x, w=None, prior=None):  # ***** or just call self.prob.adapt2multinomial ************
#     #     """Adapt concentration parameters of self.prob to given set of count data vectors
#     #     :param x: 2D-array-like sequence integer data vectors assumed to be
#     #         generated from random probability vectors drawn from self,
#     #         i.e., following a Dirichlet-Multinomial distribution, given self.alpha.
#     #         x.shape[-1] == len(self.alpha)
#     #     :param w: (optional) 1D sequence of weight values for observed data
#     #         len(w) == len(x)
#     #     :param prior: (optional) prior distribution of concentration parameters
#     #     :return: None
#     #     Result: self.alpha adapted to data and prior
#     #     Method: find MAP point estimate of concentration parameters
#     #     """
#     #     x = np.array(x)
#     #     if w is None:
#     #         w = np.ones(len(x))
#     #     n_resp = self.size
#     #     bounds = [(0.000001, None) for _ in range(n_resp)]
#     #     # to prevent inf log-likelihood
#     #     res = minimize(fun=_neg_logprob_conc,
#     #                    jac=_d_neg_logprob_conc,
#     #                    args=(x, w, prior),
#     #                    bounds=bounds,
#     #                    x0=self.prob.alpha.reshape(-1))
#     #     if res.success:
#     #         self.prob.alpha = res.x.reshape(self.shape)
#     #         return - res.fun
#     #     else:
#     #         logger.error(f'MultinomialVector.adapt: minimize res= {res}')
#     #         raise RuntimeError('MAP search did not converge. *** Should not happen ***')
#

# --------------------------------------------------- general module functions
def log_multi_beta(a, axis=-1):
    """Log multivariate beta function
    = log normalization factor for Dirichlet distribution
    :param a: array-like concentration parameter ROW vector
        or array-like sequence of such row vectors
    :param axis: (optional) int or tuple of ints for axis to be reduced
    :return: log_multi_beta =  log-normalization value(s)
        log_multi_beta.shape == a.shape[:-1]
    """
    return (np.sum(gammaln(a), axis=axis, keepdims=False)
            - gammaln(np.sum(a, axis=axis, keepdims=False)))


def d_log_multi_beta(a, axis=-1):
    """Gradient of log_multi_beta
    :param a: array-like sequence of concentration parameter vectors
    :param axis: (optional) int or tuple of ints for axis to be summed
    :return: d_log_multi_beta[..., k] = d log_multi_beta(a) / d a[..., k]
        log_multi_beta.shape == a.shape
    """
    return psi(a) - psi(np.sum(a, axis=axis, keepdims=True))


# def log_norm_multinomial(x, axis=-1):  # **** OLD, not needed for EmaCalc
#     """log normalization factor for a single Multinomial distribution
#     The multinomial probability mass for a vector x, given probability vector u
#     = (1 / c(x)) prod_k u_k^{x_k}
#     c(x) = prod_k( x_k!) / n! = prod_k gamma(x_k + 1) / gamma(n + 1),
#     where n = sum_k x_k
#     :param x: array or array-like sequence of integer counts, with all(x >= 0)
#     :param axis: (optional) int or tuple of ints for x axis to be reduced
#     :return: ln_c = array with
#         ln_c[...] = ln c(x[..., :]) if axis=-1
#         ln_c.shape == x.shape excluding given axis
#     """
#     # **** Must be method of MultinomialVector, so it knows the actual shape ??? *****************
#     x = np.asarray(x)
#     return np.sum(gammaln(1 + x), axis=-1) - gammaln(1 + np.sum(x, axis=-1))  # ************* like DirichletVector !!!


# --------------------------------------------------- local module helper stuff
def _cov_dirichlet(a):
    """DirichletVector covariance, given concentration.
    Called recursively in case of multi-dimensional concentration array
    :param a: multi-dim array of concentration ROW vectors
    :return: c = array of square covariance matrices,
        one for each row vector in a.
        c.shape == (a.shape[:-1], a.shape[-1], a.shape[-1]
    """
    if a.ndim == 1:
        asum = np.sum(a)
        c = - a[:, np.newaxis] * a  # off-diagonal
        cd = a * (asum - a)  # diagonal
        cd_ind = np.diag_indices_from(c)
        c[cd_ind] = cd
        return c / (asum**2 * (asum + 1))
    else:
        return np.array([_cov_dirichlet(a_i) for a_i in a])


def _neg_logprob_conc(a, x, w, conc_prior):  # ****** local or static class method ? ******************
    """Objective function to optimize Dirichlet concentration parameters
    using observed multinomial data
    :param a: 1D array with tentative concentration parameters
    :param x: 2D array with observed count profiles, externally shaped as
        x[n, k] = k-th element of n-th observed count profile
        x.shape[-1:] == a.shape
    :param w: 1D array with weight factors in (0, 1.)
        len(w) == x.shape[0] == (N,)
    :param conc_prior: a ConcVector instance
    :return: nlp = scalar = - w-weighted average of ln p(x_n | a),
        un-normalized because x remains fixed during learning.
    """
    # a[k] = k-th element of a, for x[n, k]
    # log_multi_beta(a).shape == ()
    # log_multi_beta(a + x).shape == (len(x),)
    lp = (np.dot(w, log_multi_beta(a) - log_multi_beta(a + x))
          # - prior_logprob_conc(a))
          - conc_prior.logprob(a))
    # lp = scalar
    return lp


def _d_neg_logprob_conc(a, x, w, conc_prior):
    """Gradient of _neg_logprob_conc w.r.t given vector of concentration parameters.
    :param a: 1D array with tentative concentration parameters
    :param x: 2D array with observed count profiles, externally shaped as
        x[n, k] = k-th element of n-th observed count profile
        x.shape[-1:] == a.shape
    :param w: 1D array with weight factors in (0, 1.)
        len(w) == x.shape[0] == (N,)
    :param conc_prior: a ConcVector instance
    :return: d_nlp = 1D array with
        d_nlp[k] = d _neg_logprob_conc(a, x, w) / d_a[k]
        d_nlp.shape == a.shape
    """
    # d_log_multi_beta(a).shape == a.shape
    # d_log_multi_beta(a + x).shape == x.shape
    dlp = (np.dot(w, d_log_multi_beta(a) - d_log_multi_beta(a + x))
           - conc_prior.d_logprob(a))
           # - d_prior_logprob_conc(a))
    # dlp.shape == a.shape
    return dlp


def _equivalent_conc(s, a_ref, s_ref):
    """Estimate a prior conc value for array size s,
    with clustering behavior similar to conc=a_ref for size s_ref
    :param s: size of desired DirichletVector object
    :param a_ref: scalar conc value that is OK for reference size = k
    :param s_ref: scalar integer size of reference DirichletVector
    :return: a = scalar solution to equation
    multi_beta(a, s) / multi_beta(1, s) == multi_beta(a_ref, s_ref) / multi_beta(1, s_ref)
    """
    def fcn(a):
        """Criterion function to solve equation fcn(a) == 0
        :param a: tentative scalar conc value
        :return: scalar function value for which zero is found
        """
        return log_multi_beta(a * np.ones(s)) - fcn_0
    # -----------------------------------------------
    fcn_ref = log_multi_beta(a_ref * np.ones(s_ref)) - log_multi_beta(np.ones(s_ref))
    fcn_0 = log_multi_beta(np.ones(s)) + fcn_ref
    if a_ref < 1.:
        a_min = 0.00001
        a_max = 1.
    else:
        a_min = 0.00001
        a_max = 2 * a_ref
    (a, res) = brentq(fcn, a_min, a_max, full_output=True)
    if res.converged:
        return a
    else:
        raise RuntimeError(f'brentq failed: {res}')

# def _equivalent_conc(s, a, k):
#     """
#     Estimate a prior conc value for array size s,
#     with clustering behavior similar to conc=a for size k
#     :param s: size of desired DirichletVector object
#     :param a: scalar conc value that is OK for reference size = k
#     :param k: scalar integer size of reference DirichletVector
#     :return: alpha = scalar
#     NOTE: This version simply inverse proportional to size
#     """
#     return a * k / s


# --------------------------------------------------------------- TEST:
if __name__ == '__main__':

    from scipy.optimize import approx_fprime, check_grad
    from EmaCalc import ema_logging
    ema_logging.setup()

    len_conc = 4  # length of concentration vector(s)

    # --------------------------------------------
    # print('*** Testing DEFAULT_CONC_PRIOR vs prior_logprob_conc')  # OK *********
    # test_conc = np.arange(len_conc) + 1.
    # print(f'prior_logprob_conc= {prior_logprob_conc(test_conc)}. '
    #       + f'DEFAULT_CONC_PRIOR.logprob= {DEFAULT_CONC_PRIOR.logprob(test_conc)}')

    # --------------------------------------------
    print('*** Testing log_multi_beta behavior')
    nx = 5
    nu_prime = 0.5

    def log_l(a, nx, nu):
        """log prob multinomial as function of uniform conc and uniform obs
        :param a: tentative scalar conc value
        :param nx: scalar size of vector
        :param nu: scalar tentative prior param
        :return: log p(a | nx, nu) non-normalized
        """
        a_vec = np.ones(nx)
        a_vec[0] = a
        return (log_multi_beta(a_vec + nu)
                - log_multi_beta(a_vec))
    # --------------------------------------------------

    a = np.linspace(0.1, 5., 50)
    lp_a = np.array([log_l(a_i, nx, nu_prime) for a_i in a])
    print('   a= ', np.array2string(a, precision=3))
    print('lp_a= ', np.array2string(lp_a, precision=3))

    # --------------------------------------------
    print('*** Testing gradient of log_multi_beta')

    # ----------------------------------------
    def test_ln_B(a):
        return log_multi_beta(a)

    def test_d_ln_B(a):
        return d_log_multi_beta(a)
    # -----------------------------------------

    test_conc = np.arange(len_conc) + 1.
    print(f'test_conc = {test_conc}')
    print(f'test_ln_B(test_conc) = {test_ln_B(test_conc)}')
    print(f'test_ln_B([test_conc]) = {test_ln_B([test_conc])}')
    print('test_d_ln_B =', test_d_ln_B(test_conc))

    err = check_grad(test_ln_B, test_d_ln_B, test_conc)
    print('approx_grad = ', approx_fprime(test_conc,
                                          test_ln_B,
                                          epsilon=1e-6))
    print('check_grad err = ', err)

    # # --------------------------------------------
    # print('*** Testing gradient of _neg_logprob_conc')
    # DEFAULT_CONC_PRIOR = ConcVector.initialize(size=len_conc)
    #
    # # ----------------------------------------
    # def test_neg_logprob_conc(a):
    #     # aa = np.tile(a.reshape((1, -1)), (10, 1))
    #     # x must be broadcast-compatible with a
    #     return _neg_logprob_conc(a, x, w, DEFAULT_CONC_PRIOR)
    #
    # def test_d_neg_logprob_conc(a):
    #     # aa = np.tile(a.reshape((1, -1)), (10, 1))
    #     return _d_neg_logprob_conc(a, x, w, DEFAULT_CONC_PRIOR)
    # # -----------------------------------------
    #
    # test_conc = np.arange(len_conc) + 1.
    # # test_conc_2D = test_conc.reshape((1, -1))
    # nx = 5
    # x = np.random.default_rng().integers(5, 10, size=(nx, len_conc))
    # # x = x[:, None, ...]  # to make it broadcast-compatible with test_conc
    # w = np.ones(nx) * 0.3
    #
    # # print(f'test_conc_2D = {test_conc_2D}')
    # print(f'x = {x}')
    # print(f'w = {w}')
    #
    # # print(f'_neg_logprob_conc(test_conc) = {_neg_logprob_conc(test_conc_2D, x, w)}')
    # # print('_d_neg_logprob_conc =', _d_neg_logprob_conc(test_conc_2D, x, w))
    #
    # print('test_neg_logprob_conc =', test_neg_logprob_conc(test_conc))
    # print('test_d_neg_logprob_conc =', test_d_neg_logprob_conc(test_conc))
    # print('approx_grad = ', approx_fprime(test_conc,
    #                                       test_neg_logprob_conc,
    #                                       epsilon=1e-6))
    # err = check_grad(test_neg_logprob_conc, test_d_neg_logprob_conc, test_conc)
    # print('check_grad err = ', err)

    # --------------------------------------------
    print('\n*** Testing DirichletVector')
    from scipy.stats import dirichlet as scipy_dir

    test_conc = np.array([1., 2., 3., 4.] )
    # test_conc = np.ones(5)

    drv = DirichletVector(test_conc)
    print(f'drv = {drv}')
    print(f'drv.alpha = {drv.alpha}')
    print(f'drv.mean = {drv.mean}')
    if drv.ndim == 1:
        print(f'scipy_dir.mean = {scipy_dir(alpha=test_conc).mean()}')
    print(f'mean(drv.rvs(size=1000) = {np.mean(drv.rvs(size=1000), axis=0)}')

    print(f'\ndrv.mean_root = {drv.mean_root}')
    print(f'sqrt(drv.mean) = {np.sqrt(drv.mean)}')
    print(f'mean(sqrt((drv.rvs(size=1000)) = {np.mean(np.sqrt(drv.rvs(size=1000)), axis=0)}')

    h2 = drv.mean_square_hellinger(drv)
    print(f'\ndrv.mean_square_hellinger(drv) = {h2}')
    r_u = np.sqrt(drv.rvs(size=1000))
    r_v = np.sqrt(drv.rvs(size=1000))
    print(f'mean_sample(1 - dot(sqrt(v),sqrt(v))) = {1. - np.mean(np.sum(r_u * r_v, axis=-1))}')
    print(f'angle Hellinger= {np.arccos(1 - h2) * 180 / np.pi:.1f} degrees')

    print(f'\ndrv.var = {drv.var}')
    if drv.ndim == 1:
        print(f'scipy_dir.var = {scipy_dir(alpha=test_conc).var()}')
    print(f'var(drv.rvs(size=1000) = {np.var(drv.rvs(size=1000), axis=0)}')
    print(f'drv.cov = {drv.cov}')
    print(f'drv.mean_log = {drv.mean_log}')
    print(f'mean(log(drv.rvs(size=1000)) = {np.mean(np.log(drv.rvs(size=1000)), axis=0)}')
    x = drv.rvs()
    print(f'x = drv.rvs() = {x}')
    print(f'drv.logpdf(x) = {drv.logpdf(x)}')
    if drv.ndim == 1:
        print(f'scipy_dir.logpdf(x) = {scipy_dir.logpdf(x, alpha=test_conc)}')
    x = drv.rvs(size=1)
    print(f'x=drv.rvs(size=1) = {x}')
    print(f'drv.logpdf(x) = {drv.logpdf(x)}')
    # print(f'scipy_dir.logpdf(x) = {scipy_dir.logpdf(x, alpha=test_conc)}')  # not allowed
    x = drv.rvs(size=(2,3))
    print(f'x = drv.rvs(size=(2,3) = {x}')
    print(f'sum(x) = {np.sum(x, axis=-1)}')
    print(f'drv.logpdf(x) = {drv.logpdf(x)}')
    # print(f'scipy_dir.logpdf(x) = {scipy_dir.logpdf(x, alpha=test_conc)}')  # not allowed
    x[0, 0, 0] += 0.001  # test outside DirichletVector support
    print(f'sum(x) = {np.sum(x, axis=-1)}')
    print(f'drv.logpdf(x) = {drv.logpdf(x)}')

    # --------------------------------------------
    # print('\n*** Testing DirichletVector.adapt2multinomial')
    # test_conc = [1., 2., 3., 4., 5.]
    # test_conc = np.array(test_conc) * 0.1
    # drv_gen = DirichletVector(alpha=test_conc)
    # nx = 100
    # u = drv_gen.rvs(size=nx)
    # x = [np.random.default_rng().multinomial(n=10, pvals=u_i)
    #      for u_i in u]
    # # print('training x= ', x)
    # drv = DirichletVector(alpha=0.5 * np.ones(len(test_conc)))
    # drv.adapt2multinomial(x)
    # print('drv.alpha= ', drv.alpha)

    # # --------------------------------------------
    # print('\n*** Testing DirichletVector')
    #
    # test_conc = [1., 2., 3., 4.]
    #
    # drv1 = DirichletVector(test_conc)
    # print(f'drv = {drv}')
    # print(f'drv.alpha = {drv.alpha}')
    # print(f'drv.mean = {drv.mean}')
    #
    # test_conc_m = np.ones((3,2, 4)) * test_conc
    # drvm = DirichletVector(test_conc_m)
    # print('drvm= ', drvm)
    # print(f'drvm.alpha = {drvm.alpha}')
    # print(f'drvm.mean = {drvm.mean}')
    # print(f'drvm.mean_log = {drvm.mean_log}')
    #
    # u = drvm.rvs(size=2)
    # print('u.shape= ', u.shape, 'u=\n', u)
    # print('logpdf(u)= ', drvm.logpdf(u))
    # print('u[0, 0, 0, :])= ', u[0, 0, 0, :])
    # print('logpdf(u[0, 0, 0,:])= ', drvm.drv[0].logpdf(u[0, 0, 0, :]))






